{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from services.preprocess import preprocess_model,preprocess_lemma\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(df_intents,df_patterns,path):\n",
    "    classes = [j['tag'] for i,j in df_intents.iterrows()]\n",
    "    words,documents = preprocess_model(df_patterns,df_intents)\n",
    "\n",
    "    # classificamos nossas listas\n",
    "    words = sorted(list(set(words)))\n",
    "    classes = sorted(list(set(classes)))\n",
    "\n",
    "    # salvamos as palavras e classes nos arquivos pkl\n",
    "    words_path = os.path.join(path, \"words.pkl\")\n",
    "    classes_path = os.path.join(path, \"classes.pkl\")\n",
    "\n",
    "    pickle.dump(words,open(words_path, 'wb'))\n",
    "    pickle.dump(classes,open(classes_path, 'wb'))\n",
    "\n",
    "    # inicializamos o treinamento\n",
    "    training = []\n",
    "    output_empty = [0] * len(classes)\n",
    "    for document in documents:\n",
    "        # inicializamos o saco de palavras \n",
    "        bag = []\n",
    "\n",
    "        # listamos as palavras do pattern\n",
    "        pattern_words = document[0]\n",
    "\n",
    "        # lematizamos cada palavra \n",
    "        # na tentativa de representar palavras relacionadas\n",
    "        pattern_words = [preprocess_lemma(word.lower()) for word in pattern_words]\n",
    "\n",
    "        # criamos nosso conjunto de palavras com 1, \n",
    "        # se a correspondência de palavras for encontrada no padrão atual\n",
    "        for word in words:\n",
    "            bag.append(1) if word in pattern_words else bag.append(0)\n",
    "\n",
    "        # adicionamos zeros à lista para preencher com o tamanho máximo\n",
    "        # de palavras no vocabulário\n",
    "        while len(bag) < len(words):\n",
    "            bag.append(0)\n",
    "\n",
    "        # output_row atuará como uma chave para a lista, \n",
    "        # onde a saida será 0 para cada tag e 1 para a tag atual\n",
    "        output_row = list(output_empty)\n",
    "        output_row[classes.index(document[1])] = 1\n",
    "\n",
    "        training.append([bag, output_row])\n",
    "\n",
    "    # embaralhamos nosso conjunto de treinamentos e transformamos em numpy array\n",
    "    random.shuffle(training)\n",
    "    training = np.array(training, dtype=object)\n",
    "    # criamos lista de treino sendo x os patterns e y as intenções\n",
    "    x = list(training[:, 0])\n",
    "    y = list(training[:, 1])\n",
    "\n",
    "    # Criamos nosso modelo com 3 camadas. \n",
    "    # Primeira camada de 128 neurônios, \n",
    "    # segunda camada de 64 neurônios e terceira camada de saída \n",
    "    # contém número de neurônios igual ao número de intenções para prever a intenção de saída com softmax\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(x[0]),), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(y[0]), activation='softmax'))\n",
    "\n",
    "    # O modelo é compilado com descida de gradiente estocástica \n",
    "    # com gradiente acelerado de Nesterov.\n",
    "    # A ideia da otimização do Momentum de Nesterov, ou Nesterov Accelerated Gradient (NAG), \n",
    "    # é medir o gradiente da função de custo não na posição local,\n",
    "    # mas ligeiramente à frente na direção do momentum. \n",
    "    # A única diferença entre a otimização de Momentum é que o gradiente é medido em θ + βm em vez de em θ.\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    # ajustamos e salvamos o modelo\n",
    "    m = model.fit(np.array(x), np.array(y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "    model_path = os.path.join(path, \"model.h5\")\n",
    "    model.save(model_path, m)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
