{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "from pickle import dump, load\n",
    "from numpy import array\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD\n",
    "from re import sub\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk import word_tokenize\n",
    "import language_tool_python\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_model(dict):\n",
    "    words = []\n",
    "    documents = []\n",
    "    for chave, valor in dict.items():\n",
    "        for i in valor:\n",
    "            words.append(i)\n",
    "            documents.append((i, chave))\n",
    "    return words, documents"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(dict):\n",
    "    classes = []\n",
    "    classes.extend(list(dict.keys()))\n",
    "    words,documents = preprocess_model(dict)\n",
    "\n",
    "    words = sorted(list(set(words)))\n",
    "    classes = sorted(list(set(classes)))\n",
    "\n",
    "    words_path = (\"words.pkl\")\n",
    "    classes_path = (\"classes.pkl\")\n",
    "\n",
    "    dump(words,open(words_path, 'wb'))\n",
    "    dump(classes,open(classes_path, 'wb'))\n",
    "\n",
    "    training = []\n",
    "    output_empty = [0] * len(classes)\n",
    "    for document in documents:\n",
    "        bag = []\n",
    "        pattern_words = document[0]\n",
    "        for word in words:\n",
    "            bag.append(1) if word in pattern_words else bag.append(0)\n",
    "        while len(bag) < len(words):\n",
    "            bag.append(0)\n",
    "        output_row = list(output_empty)\n",
    "        output_row[classes.index(document[1])] = 1\n",
    "        training.append([bag, output_row])\n",
    "    shuffle(training)\n",
    "    training = array(training, dtype=object)\n",
    "\n",
    "    x = list(training[:, 0])\n",
    "    y = list(training[:, 1])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(x[0]),), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(classes), activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    m = model.fit(array(x), array(y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "    model_path = (\"model.h5\")\n",
    "    model.save(model_path, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'..\\portugues\\pt_troubleshooting.xlsx')\n",
    "df_pattern = pd.read_excel(r'..\\portugues\\pt_patterns.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict = {}\n",
    "for r in range(len(df['problem'].unique())):\n",
    "    dict[str(r)] = df_pattern.loc[r,'patterns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retorna 0 ou 1 para cada palavra da bolsa de palavras\n",
    "def bag_of_words(writing, words):\n",
    "    sentence_words = writing.split()\n",
    "    # cria uma matriz de N palavras\n",
    "    bag = [0]*len(words)\n",
    "    for setence in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == setence:\n",
    "                # atribui 1 no pacote de palavra se a palavra atual estiver na posição da frase\n",
    "                bag[i] = 1\n",
    "    return(array(bag))\n",
    "\n",
    "def class_prediction(input_user):\n",
    "    model = load_model('model.h5')\n",
    "    words = load(open('words.pkl', 'rb'))\n",
    "    classes = load(open('classes.pkl', 'rb'))\n",
    "    # filtra as previsões abaixo de um limite 0.25\n",
    "    prevision = bag_of_words(input_user, words)\n",
    "    response_prediction = model.predict(array([prevision]))[0]\n",
    "    results = [[index, response] for index, response in enumerate(response_prediction)]\n",
    "\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [{\"suggestion\": classes[r[0]], \"probability\": str(r[1])} for r in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_semantic(frase):\n",
    "    tool = language_tool_python.LanguageTool('pt')\n",
    "    matches = tool.check(frase)\n",
    "    for i in matches:\n",
    "        frase = frase[:i.offset] + i.replacements[0] + frase[i.offset+i.errorLength:]\n",
    "    tool.close()\n",
    "    return frase\n",
    "\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = [stemmer.stem(token) for token in tokens]\n",
    "    text = ' '.join([str(element) for element in stems])\n",
    "    return text\n",
    "\n",
    "def preprocess_input(text):\n",
    "    text = preprocess_semantic(text)\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = text.lower().strip()\n",
    "    # tirar pontuações, acentos e espaços extras\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    # tirar espaços em branco\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texto = 'fortes vibrações durante a operação da bomba'\n",
    "texto = preprocess_input(texto)\n",
    "lista = texto.split()\n",
    "for i in lista:\n",
    "    response = class_prediction(i)\n",
    "    print(i)\n",
    "    print(response)\n",
    "    print()\n",
    "# max_value = 0\n",
    "# classe = ''\n",
    "# for i in response:\n",
    "#     for j in i:\n",
    "#         if 'e' not in i[\"probability\"] or '-' not in i[\"probability\"]:\n",
    "#             value = float(i[\"probability\"])\n",
    "#             if value > float(max_value):\n",
    "#                 max_value = value\n",
    "#                 classe = i"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
