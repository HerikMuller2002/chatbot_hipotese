{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from re import sub\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = sub(r'\\d+', '', text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return keywords\n",
    "\n",
    "def get_synonyms(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    synonyms = []\n",
    "    for word in tokens:\n",
    "        for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "            for lemma in syn.lemmas(lang=\"por\"):\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'..\\portugues\\pt_troubleshooting.xlsx')v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rows = []\n",
    "for row in df['problem'].unique():\n",
    "    list_words = []\n",
    "    row = remove_num(row)\n",
    "    row = remove_punct(row)\n",
    "    list_keywords = extract_keywords(row)\n",
    "    for keyword in list_keywords:                \n",
    "        x = remove_accent(keyword)\n",
    "        if x not in list_words:\n",
    "            list_words.append(x)\n",
    "\n",
    "        x = preprocess_lemma(x)\n",
    "        if x not in list_words:\n",
    "            list_words.append(x)\n",
    "            \n",
    "        x = preprocess_stem(x)\n",
    "        if x not in list_words:\n",
    "            list_words.append(x)\n",
    "        \n",
    "        z = get_synonyms(keyword)\n",
    "        for synonym in z:\n",
    "            synonym = remove_punct(synonym)\n",
    "            x = remove_accent(synonym)\n",
    "            if x not in list_words:\n",
    "                list_words.append(x)\n",
    "\n",
    "            x = preprocess_lemma(x)\n",
    "            if x not in list_words:\n",
    "                list_words.append(x)\n",
    "                \n",
    "            x = preprocess_stem(x)\n",
    "            if x not in list_words:\n",
    "                list_words.append(x)\n",
    "    list_rows.append(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>patterns</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bomba, bomb, Bombas, bomba d agua, bomb d agu,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bomba, bomb, Bombas, bomba d agua, bomb d agu,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fortes, fort, vibraçoes, vibraço, Vibraçao, vi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vibraçoes, vibraço, Vibraçao, vibraça, oscilaç...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>capacidade, capac, Incompetencia, incompetenc,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>alta, alto, berrante, berrant, estrepitante, e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>falha, falh, insucesso, insucess, erro, fracas...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>fumaça, fumac, fumo, fum, fumos, gas de fornal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>isolamento, isol, insulaçao, insulaça, incomun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>disjuntores, disjuntor, fusiveis, fusiv, abert...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>151 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              patterns\n",
       "0    bomba, bomb, Bombas, bomba d agua, bomb d agu,...\n",
       "1    bomba, bomb, Bombas, bomba d agua, bomb d agu,...\n",
       "2    fortes, fort, vibraçoes, vibraço, Vibraçao, vi...\n",
       "3    vibraçoes, vibraço, Vibraçao, vibraça, oscilaç...\n",
       "4    capacidade, capac, Incompetencia, incompetenc,...\n",
       "..                                                 ...\n",
       "146  alta, alto, berrante, berrant, estrepitante, e...\n",
       "147  falha, falh, insucesso, insucess, erro, fracas...\n",
       "148  fumaça, fumac, fumo, fum, fumos, gas de fornal...\n",
       "149  isolamento, isol, insulaçao, insulaça, incomun...\n",
       "150  disjuntores, disjuntor, fusiveis, fusiv, abert...\n",
       "\n",
       "[151 rows x 1 columns]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(list_rows)):\n",
    "    list_rows[i] = [', '.join(list_rows[i])]\n",
    "new_df = pd.DataFrame(list_rows, columns=['patterns'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_excel(r'..\\portugues\\pt_patterns.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
