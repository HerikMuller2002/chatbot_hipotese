{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_key = \"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\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotulagem(txt):\n",
    "  req = r\"estou desenvolvendo um modelo de NER para identificar equipamento em um texto do usuário, contexto o cliente irá abrir um chamado descrevendo o problema e o modelo deve identificar o equipamento. para isso preciso de alguns dados rotulados... para isso vou te dar um dicionário python python onde cada key representa o equipamento e os values são exemplos dos problemas, quero que vc retorne para mim essas dados rotulados no seguinte exemplo:'O meu computador está com um problema no teclado.', 'entities': (6, 16,'EQUIPAMENTO'). Resumindo, rotule um conjunto de exemplos de problemas para cada equipamento especificado no dicionário: {}. retorne no seguinte format: dicionário python no seguinte formato: {'equipament':equipamento..., 'problem':lista de problemas..., 'rotulo':lista de rotulos para cada problema}\".format(txt)\n",
    "  completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": req}\n",
    "    ]\n",
    "  )\n",
    "  res = completion.choices[0].message.content\n",
    "  return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'..\\portugues\\pt_troubleshooting.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "\n",
    "def similares(frase, equipamento):\n",
    "    palavras_similares = {}\n",
    "    equipamento = equipamento.lower()\n",
    "    palavras_frase = frase.lower().split()\n",
    "\n",
    "    for palavra in palavras_frase:\n",
    "        similaridade = fuzz.ratio(palavra, equipamento)\n",
    "        if similaridade >= 80:  # Ajuste esse valor de acordo com a sua necessidade\n",
    "            palavras_similares[palavra] = {\"similaridade\": similaridade, \"tag\": \"equipamento\"}\n",
    "\n",
    "    return palavras_similares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "cont = 0\n",
    "for equip in df['equipament'].unique():\n",
    "    cont += 1\n",
    "    if equip != 'Motores elétricos (dc)':\n",
    "        filter_df = df.loc[df['equipament'] == equip]\n",
    "        lista = [pr for pr in filter_df['problem'].unique()]\n",
    "        dict = {'equipament': f'class{cont}', 'problems': lista}\n",
    "        problemas_bombas = [    \"A bomba não está ligando\",    \"A bomba está superaquecendo\",    \"A bomba está apresentando baixo fluxo de líquido\",    \"A bomba está vibrando excessivamente\",    \"A bomba está consumindo muita energia elétrica\"]\n",
    "        for j in problemas_bombas:\n",
    "            s = similares(j,equip)\n",
    "            print(j)\n",
    "            print(s)\n",
    "            print()\n",
    "            \n",
    "        # for j in lista:\n",
    "        #     s = similares(j,equip)\n",
    "        #     print(j)\n",
    "        #     print(s)\n",
    "        #     print()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gpt_2_simple as gpt2\n",
    "\n",
    "gpt2.download_gpt2()\n",
    "\n",
    "# Carregue o modelo\n",
    "sess = gpt2.start_tf_sess()\n",
    "gpt2.load_gpt2(sess)\n",
    "\n",
    "# Gere texto com o modelo usando um tópico\n",
    "topic = \"Inteligência Artificial\"\n",
    "text = gpt2.generate(sess, length=100, temperature=0.7, prefix=topic, return_as_list=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprima o texto gerado\n",
    "print(text[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def create_json(json_path, content):\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content, f, indent=4)\n",
    "\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_df(file):\n",
    "    df = pd.read_excel(file)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    stems = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return str(stems)\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punct(text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = remove_accent(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def encontrar_palavras(texto, frases):\n",
    "    resultado = []\n",
    "    \n",
    "    for frase in frases:\n",
    "        padrao = re.compile(r'\\b{}\\b'.format(re.escape(frase)))\n",
    "        correspondencias = padrao.finditer(texto)\n",
    "        \n",
    "        for correspondencia in correspondencias:\n",
    "            dicionario = {\n",
    "                \"text\": frase,\n",
    "                \"startc\": correspondencia.start(),\n",
    "                \"endc\": correspondencia.end() - 1,\n",
    "                \"startp\": len(re.findall(r'\\b\\w+\\b', texto[:correspondencia.start()])),\n",
    "                \"endp\": (len(re.findall(r'\\b\\w+\\b', texto[:correspondencia.start()])) + len(frase.split())) - 1\n",
    "            }\n",
    "            resultado.append(dicionario)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(r'..\\documents\\entidades2.json')\n",
    "df = load_df('..\\documents\\classes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "new_data = []\n",
    "words = []\n",
    "for idx, row in df.iterrows():\n",
    "    classe = idx + 1\n",
    "    try:\n",
    "        if data[idx]['classe'] == classe:\n",
    "            list_words = row['keywords'].split(',')\n",
    "            for i in list_words:\n",
    "                i = preprocess(i).strip()\n",
    "                words.append(i)\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "for i in range(len(data)):\n",
    "    new_dict = {f\"classe\":data[i]['classe']}\n",
    "    cont = 0\n",
    "    for texto in data[i][\"texts\"]:\n",
    "        texto = preprocess(texto).strip()\n",
    "        dict_ = encontrar_palavras(texto,words)\n",
    "        new_dict[f'texto{cont}'] = texto\n",
    "        new_dict['entities'] = dict_\n",
    "        cont += 1\n",
    "        new_data.append(new_dict)\n",
    "        print(new_data)\n",
    "\n",
    "# create_json(r'..\\documents\\entidades2-classify.json',new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
