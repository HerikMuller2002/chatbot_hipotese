{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "[E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m nlp \u001b[39m=\u001b[39m spacy\u001b[39m.\u001b[39;49mload(\u001b[39m\"\u001b[39;49m\u001b[39mpt_core_news_sm\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chatbot_hipotese\\.venv\\lib\\site-packages\\spacy\\__init__.py:54\u001b[0m, in \u001b[0;36mload\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload\u001b[39m(\n\u001b[0;32m     31\u001b[0m     name: Union[\u001b[39mstr\u001b[39m, Path],\n\u001b[0;32m     32\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     37\u001b[0m     config: Union[Dict[\u001b[39mstr\u001b[39m, Any], Config] \u001b[39m=\u001b[39m util\u001b[39m.\u001b[39mSimpleFrozenDict(),\n\u001b[0;32m     38\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Language:\n\u001b[0;32m     39\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Load a spaCy model from an installed package or a local path.\u001b[39;00m\n\u001b[0;32m     40\u001b[0m \n\u001b[0;32m     41\u001b[0m \u001b[39m    name (str): Package name or model path.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39m    RETURNS (Language): The loaded nlp object.\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 54\u001b[0m     \u001b[39mreturn\u001b[39;00m util\u001b[39m.\u001b[39;49mload_model(\n\u001b[0;32m     55\u001b[0m         name,\n\u001b[0;32m     56\u001b[0m         vocab\u001b[39m=\u001b[39;49mvocab,\n\u001b[0;32m     57\u001b[0m         disable\u001b[39m=\u001b[39;49mdisable,\n\u001b[0;32m     58\u001b[0m         enable\u001b[39m=\u001b[39;49menable,\n\u001b[0;32m     59\u001b[0m         exclude\u001b[39m=\u001b[39;49mexclude,\n\u001b[0;32m     60\u001b[0m         config\u001b[39m=\u001b[39;49mconfig,\n\u001b[0;32m     61\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chatbot_hipotese\\.venv\\lib\\site-packages\\spacy\\util.py:449\u001b[0m, in \u001b[0;36mload_model\u001b[1;34m(name, vocab, disable, enable, exclude, config)\u001b[0m\n\u001b[0;32m    447\u001b[0m \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m OLD_MODEL_SHORTCUTS:\n\u001b[0;32m    448\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE941\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname, full\u001b[39m=\u001b[39mOLD_MODEL_SHORTCUTS[name]))  \u001b[39m# type: ignore[index]\u001b[39;00m\n\u001b[1;32m--> 449\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mIOError\u001b[39;00m(Errors\u001b[39m.\u001b[39mE050\u001b[39m.\u001b[39mformat(name\u001b[39m=\u001b[39mname))\n",
      "\u001b[1;31mOSError\u001b[0m: [E050] Can't find model 'pt_core_news_sm'. It doesn't seem to be a Python package or a valid path to a data directory."
     ]
    }
   ],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(data_dict, iterations):\n",
    "    nlp = spacy.blank(\"pt\")\n",
    "\n",
    "    nlp.add_pipe(\"ner\", name=\"ner\", last=True)\n",
    "\n",
    "    for label in data_dict.keys():\n",
    "        nlp.get_pipe(\"ner\").add_label(label)\n",
    "\n",
    "    train_data = []\n",
    "    for label, examples in data_dict.items():\n",
    "        for text, annotations in examples:\n",
    "            train_data.append((text, annotations))\n",
    "\n",
    "    nlp.begin_training()\n",
    "    for itn in range(iterations):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        \n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example_batch = []\n",
    "            for text, annotation in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = spacy.training.example.Example.from_dict(doc, annotation)\n",
    "                example_batch.append(example)\n",
    "            nlp.update(example_batch, losses=losses)\n",
    "        \n",
    "        print(\"Iteration:\", itn+1, \"Loss:\", losses)\n",
    "\n",
    "    return nlp"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de dados dos par√¢metros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Loss: {'ner': 1.6666666865348816}\n",
      "Iteration: 2 Loss: {'ner': 1.5900554656982422}\n",
      "Iteration: 3 Loss: {'ner': 1.4327635169029236}\n",
      "Iteration: 4 Loss: {'ner': 1.220018208026886}\n",
      "Iteration: 5 Loss: {'ner': 0.9769566357135773}\n",
      "Iteration: 6 Loss: {'ner': 0.6846418380737305}\n",
      "Iteration: 7 Loss: {'ner': 0.4011929929256439}\n",
      "Iteration: 8 Loss: {'ner': 0.16825400292873383}\n",
      "Iteration: 9 Loss: {'ner': 0.07151926681399345}\n",
      "Iteration: 10 Loss: {'ner': 0.020669556222856045}\n",
      "Iteration: 11 Loss: {'ner': 0.003666175529360771}\n",
      "Iteration: 12 Loss: {'ner': 0.0006613792502321303}\n",
      "Iteration: 13 Loss: {'ner': 0.0002842040848918259}\n",
      "Iteration: 14 Loss: {'ner': 1.965474075404927e-05}\n",
      "Iteration: 15 Loss: {'ner': 4.1652087929833215e-06}\n",
      "Iteration: 16 Loss: {'ner': 7.758024906934224e-07}\n",
      "Iteration: 17 Loss: {'ner': 9.059862549065656e-08}\n",
      "Iteration: 18 Loss: {'ner': 1.0446634401972688e-07}\n",
      "Iteration: 19 Loss: {'ner': 3.749559968069249e-09}\n",
      "Iteration: 20 Loss: {'ner': 1.5915142537181737e-09}\n",
      "Iteration: 21 Loss: {'ner': 5.0064343296019054e-11}\n",
      "Iteration: 22 Loss: {'ner': 3.673701035218635e-11}\n",
      "Iteration: 23 Loss: {'ner': 9.109034490237256e-12}\n",
      "Iteration: 24 Loss: {'ner': 5.256262244034138e-11}\n",
      "Iteration: 25 Loss: {'ner': 1.6440257290836584e-13}\n",
      "Iteration: 26 Loss: {'ner': 2.9452584576934726e-12}\n",
      "Iteration: 27 Loss: {'ner': 9.326839024411185e-14}\n",
      "Iteration: 28 Loss: {'ner': 3.8957141820176316e-13}\n",
      "Iteration: 29 Loss: {'ner': 4.665456626443739e-14}\n",
      "Iteration: 30 Loss: {'ner': 3.499186532841597e-13}\n",
      "Iteration: 31 Loss: {'ner': 2.917524095170975e-14}\n",
      "Iteration: 32 Loss: {'ner': 6.033651211391277e-15}\n",
      "Iteration: 33 Loss: {'ner': 5.061870200398811e-14}\n",
      "Iteration: 34 Loss: {'ner': 5.6665157018349644e-14}\n",
      "Iteration: 35 Loss: {'ner': 4.949240162566213e-16}\n",
      "Iteration: 36 Loss: {'ner': 4.251612527920404e-16}\n",
      "Iteration: 37 Loss: {'ner': 1.0272784912443041e-15}\n",
      "Iteration: 38 Loss: {'ner': 4.498989029561615e-16}\n",
      "Iteration: 39 Loss: {'ner': 1.9630108162934986e-16}\n",
      "Iteration: 40 Loss: {'ner': 1.0381246131380195e-14}\n",
      "Iteration: 41 Loss: {'ner': 2.3712153727627368e-15}\n",
      "Iteration: 42 Loss: {'ner': 3.893069477373124e-16}\n",
      "Iteration: 43 Loss: {'ner': 1.42796981584056e-15}\n",
      "Iteration: 44 Loss: {'ner': 1.959908640842241e-16}\n",
      "Iteration: 45 Loss: {'ner': 3.648434886936239e-15}\n",
      "Iteration: 46 Loss: {'ner': 4.557120634999905e-16}\n",
      "Iteration: 47 Loss: {'ner': 1.116776436582559e-15}\n",
      "Iteration: 48 Loss: {'ner': 2.9165154245145827e-17}\n",
      "Iteration: 49 Loss: {'ner': 1.5265514829837023e-15}\n",
      "Iteration: 50 Loss: {'ner': 1.1034346759383727e-16}\n",
      "Iteration: 51 Loss: {'ner': 1.5896530323197216e-16}\n",
      "Iteration: 52 Loss: {'ner': 1.455819193075908e-14}\n",
      "Iteration: 53 Loss: {'ner': 6.531667324265284e-18}\n",
      "Iteration: 54 Loss: {'ner': 1.0322356671956874e-17}\n",
      "Iteration: 55 Loss: {'ner': 5.208271524783974e-18}\n",
      "Iteration: 56 Loss: {'ner': 1.5769532911520674e-17}\n",
      "Iteration: 57 Loss: {'ner': 6.082675529540833e-17}\n",
      "Iteration: 58 Loss: {'ner': 1.581816387755256e-17}\n",
      "Iteration: 59 Loss: {'ner': 3.931891806657944e-17}\n",
      "Iteration: 60 Loss: {'ner': 6.165442849401613e-17}\n",
      "Iteration: 61 Loss: {'ner': 4.706802243510577e-16}\n",
      "Iteration: 62 Loss: {'ner': 2.2337093940642333e-16}\n",
      "Iteration: 63 Loss: {'ner': 7.231725998528922e-16}\n",
      "Iteration: 64 Loss: {'ner': 1.980711379700955e-17}\n",
      "Iteration: 65 Loss: {'ner': 3.504100369921066e-17}\n",
      "Iteration: 66 Loss: {'ner': 1.1941107358102215e-17}\n",
      "Iteration: 67 Loss: {'ner': 1.938166249623085e-17}\n",
      "Iteration: 68 Loss: {'ner': 4.848161631305335e-16}\n",
      "Iteration: 69 Loss: {'ner': 5.36598056286687e-17}\n",
      "Iteration: 70 Loss: {'ner': 5.349898066751943e-13}\n",
      "Iteration: 71 Loss: {'ner': 2.9324656173941977e-17}\n",
      "Iteration: 72 Loss: {'ner': 3.822148777683435e-18}\n",
      "Iteration: 73 Loss: {'ner': 3.6290859704266585e-17}\n",
      "Iteration: 74 Loss: {'ner': 6.489257210587801e-15}\n",
      "Iteration: 75 Loss: {'ner': 1.2827189143317929e-17}\n",
      "Iteration: 76 Loss: {'ner': 3.500650810278044e-17}\n",
      "Iteration: 77 Loss: {'ner': 2.3406082731378855e-17}\n",
      "Iteration: 78 Loss: {'ner': 3.870155057562957e-18}\n",
      "Iteration: 79 Loss: {'ner': 1.2567833994432575e-16}\n",
      "Iteration: 80 Loss: {'ner': 7.572849261270932e-17}\n",
      "Iteration: 81 Loss: {'ner': 2.9803502905567358e-15}\n",
      "Iteration: 82 Loss: {'ner': 3.081427254103558e-18}\n",
      "Iteration: 83 Loss: {'ner': 4.996905105331989e-17}\n",
      "Iteration: 84 Loss: {'ner': 4.251093483666556e-15}\n",
      "Iteration: 85 Loss: {'ner': 3.687356269238256e-17}\n",
      "Iteration: 86 Loss: {'ner': 4.206888394211989e-18}\n",
      "Iteration: 87 Loss: {'ner': 8.256245100449353e-18}\n",
      "Iteration: 88 Loss: {'ner': 1.6465281400677867e-17}\n",
      "Iteration: 89 Loss: {'ner': 2.345516894724685e-16}\n",
      "Iteration: 90 Loss: {'ner': 2.3566587658026756e-16}\n",
      "Iteration: 91 Loss: {'ner': 7.224484007450602e-18}\n",
      "Iteration: 92 Loss: {'ner': 1.115025710286921e-14}\n",
      "Iteration: 93 Loss: {'ner': 1.4727625600448833e-16}\n",
      "Iteration: 94 Loss: {'ner': 2.981991649846781e-16}\n",
      "Iteration: 95 Loss: {'ner': 3.2894243265212486e-17}\n",
      "Iteration: 96 Loss: {'ner': 4.338749136751854e-15}\n",
      "Iteration: 97 Loss: {'ner': 1.9614796760283415e-17}\n",
      "Iteration: 98 Loss: {'ner': 4.365064766076523e-18}\n",
      "Iteration: 99 Loss: {'ner': 2.3165433350515487e-17}\n",
      "Iteration: 100 Loss: {'ner': 8.730038128695359e-18}\n"
     ]
    }
   ],
   "source": [
    "data_dict = entidades = {\n",
    "    \"BOMBA\": [\n",
    "    (\"Bomba centr√≠fuga\", {\"entities\": [(0, 5, \"BOMBA\")]}),\n",
    "    ]}\n",
    "\n",
    "output_dir = r\"..\\documents\"\n",
    "\n",
    "iterations = 100\n",
    "model = train_model(data_dict, iterations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(json_path, content):\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content, f, indent=4)\n",
    "\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_df(file):\n",
    "    df = pd.read_excel(file)\n",
    "    return df\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    stems = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return str(stems)\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[√°√†√£√¢√§]', 'a', sub('[√©√®√™√´]', 'e', sub('[√≠√¨√Æ√Ø]', 'i', sub('[√≥√≤√µ√¥√∂]', 'o', sub('[√∫√π√ª√º]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punct(text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = remove_accent(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def encontrar_palavras(texto, frases):\n",
    "    resultado = []\n",
    "    \n",
    "    for frase in frases:\n",
    "        padrao = re.compile(r'\\b{}\\b'.format(re.escape(frase)))\n",
    "        correspondencias = padrao.finditer(texto)\n",
    "        \n",
    "        for correspondencia in correspondencias:\n",
    "            dicionario = {\n",
    "                \"text\": frase,\n",
    "                \"startc\": correspondencia.start(),\n",
    "                \"endc\": correspondencia.end() - 1,\n",
    "                \"startp\": len(re.findall(r'\\b\\w+\\b', texto[:correspondencia.start()])),\n",
    "                \"endp\": (len(re.findall(r'\\b\\w+\\b', texto[:correspondencia.start()])) + len(frase.split())) - 1\n",
    "            }\n",
    "            resultado.append(dicionario)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(r'..\\documents\\entidades2.json')\n",
    "df = load_df('..\\documents\\classes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "words = []\n",
    "for idx, row in df.iterrows():\n",
    "    classe = idx + 1\n",
    "    try:\n",
    "        if data[idx]['classe'] == classe:\n",
    "            list_words = row['keywords'].split(',')\n",
    "            for i in range(len(list_words)):\n",
    "                list_words[i] = preprocess(list_words[i]).strip()\n",
    "            words.append(list_words)\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "new_dict = {}\n",
    "for i in range(len(data)):\n",
    "    list_tuple = []\n",
    "    for texto in data[i][\"texts\"]:\n",
    "        texto = preprocess(texto).strip()\n",
    "        list_find_word = encontrar_palavras(texto,words[i])\n",
    "        list_find = []\n",
    "        for j in list_find_word:\n",
    "            list_word_found = [j['startc'],j['endc']+1,str(data[i]['classe'])]\n",
    "            list_find.append(list_word_found)\n",
    "        list_tuple.append((texto, {\"entities\": list_find}))\n",
    "    new_dict[str(data[i]['classe'])] = list_tuple\n",
    "        # dict_ = encontrar_palavras(texto,words)\n",
    "        # new_dict[f'texto{cont}'] = texto\n",
    "        # new_dict['entities'] = dict_\n",
    "        # cont += 1\n",
    "        # new_data.append(new_dict)\n",
    "        # print(new_data)\n",
    "    create_json(r'..\\documents\\entidades2-classify.json',new_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrigir_sobreposicao_entidades(data_dict):\n",
    "    for label, examples in data_dict.items():\n",
    "        for i in range(len(examples)):\n",
    "            entities = examples[i][1][\"entities\"]\n",
    "            entities_sorted = sorted(entities, key=lambda x: x[1] - x[0], reverse=True)\n",
    "            entities_filtered = []\n",
    "            for j in range(len(entities_sorted)):\n",
    "                entity = entities_sorted[j]\n",
    "                entity_start = entity[0]\n",
    "                entity_end = entity[1]\n",
    "                entity_label = entity[2]\n",
    "                \n",
    "                entity_exists = False\n",
    "                for existing_entity in entities_filtered:\n",
    "                    if (existing_entity[0] <= entity_start <= existing_entity[1]) or (existing_entity[0] <= entity_end <= existing_entity[1]):\n",
    "                        entity_exists = True\n",
    "                        break\n",
    "                \n",
    "                if not entity_exists:\n",
    "                    entities_filtered.append(entity)\n",
    "            \n",
    "            data_dict[label][i][1][\"entities\"] = entities_filtered\n",
    "    \n",
    "    return data_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Loss: {'ner': 1718.6461276270636}\n",
      "Iteration: 2 Loss: {'ner': 367.1908256192367}\n",
      "Iteration: 3 Loss: {'ner': 327.83803614774195}\n",
      "Iteration: 4 Loss: {'ner': 206.2609648051739}\n",
      "Iteration: 5 Loss: {'ner': 70.65038252161628}\n",
      "Iteration: 6 Loss: {'ner': 47.07972406327113}\n",
      "Iteration: 7 Loss: {'ner': 65.5676245116571}\n",
      "Iteration: 8 Loss: {'ner': 97.66917580153701}\n",
      "Iteration: 9 Loss: {'ner': 48.561792174187}\n",
      "Iteration: 10 Loss: {'ner': 58.35268329232744}\n",
      "Iteration: 11 Loss: {'ner': 27.516725597068636}\n",
      "Iteration: 12 Loss: {'ner': 45.275293702007154}\n",
      "Iteration: 13 Loss: {'ner': 29.091798090263243}\n",
      "Iteration: 14 Loss: {'ner': 32.93091400832715}\n",
      "Iteration: 15 Loss: {'ner': 25.606365041901142}\n",
      "Iteration: 16 Loss: {'ner': 26.592954891271624}\n",
      "Iteration: 17 Loss: {'ner': 20.300315833996876}\n",
      "Iteration: 18 Loss: {'ner': 62.497444262652856}\n",
      "Iteration: 19 Loss: {'ner': 54.406453047254736}\n",
      "Iteration: 20 Loss: {'ner': 13.691980790213547}\n",
      "Iteration: 21 Loss: {'ner': 32.71854349355182}\n",
      "Iteration: 22 Loss: {'ner': 22.96482156795719}\n",
      "Iteration: 23 Loss: {'ner': 50.959566491449564}\n",
      "Iteration: 24 Loss: {'ner': 29.717230056064665}\n",
      "Iteration: 25 Loss: {'ner': 75.379794734202}\n",
      "Iteration: 26 Loss: {'ner': 28.900124001957256}\n",
      "Iteration: 27 Loss: {'ner': 24.714789530109105}\n",
      "Iteration: 28 Loss: {'ner': 23.35821668995913}\n",
      "Iteration: 29 Loss: {'ner': 21.613535650868617}\n",
      "Iteration: 30 Loss: {'ner': 15.025455209426628}\n",
      "Iteration: 31 Loss: {'ner': 16.342152001622157}\n",
      "Iteration: 32 Loss: {'ner': 18.167264273171085}\n",
      "Iteration: 33 Loss: {'ner': 30.71025009424067}\n",
      "Iteration: 34 Loss: {'ner': 25.239207625902175}\n",
      "Iteration: 35 Loss: {'ner': 69.98512080520086}\n",
      "Iteration: 36 Loss: {'ner': 24.08537373890487}\n",
      "Iteration: 37 Loss: {'ner': 12.786130754355344}\n",
      "Iteration: 38 Loss: {'ner': 12.610729234717905}\n",
      "Iteration: 39 Loss: {'ner': 34.841865169813076}\n",
      "Iteration: 40 Loss: {'ner': 68.07333201209843}\n",
      "Iteration: 41 Loss: {'ner': 24.94512402037988}\n",
      "Iteration: 42 Loss: {'ner': 14.895759995171572}\n",
      "Iteration: 43 Loss: {'ner': 10.568871917883682}\n",
      "Iteration: 44 Loss: {'ner': 18.30868881766458}\n",
      "Iteration: 45 Loss: {'ner': 7.127193062734209}\n",
      "Iteration: 46 Loss: {'ner': 13.220149131011633}\n",
      "Iteration: 47 Loss: {'ner': 28.307088943539547}\n",
      "Iteration: 48 Loss: {'ner': 16.747074107703284}\n",
      "Iteration: 49 Loss: {'ner': 62.8732651224951}\n",
      "Iteration: 50 Loss: {'ner': 20.53072576126395}\n",
      "Iteration: 51 Loss: {'ner': 11.859070591172314}\n",
      "Iteration: 52 Loss: {'ner': 15.935348952659806}\n",
      "Iteration: 53 Loss: {'ner': 9.104573417068137}\n",
      "Iteration: 54 Loss: {'ner': 28.31884071044456}\n",
      "Iteration: 55 Loss: {'ner': 30.893972055055446}\n",
      "Iteration: 56 Loss: {'ner': 18.674689389858848}\n",
      "Iteration: 57 Loss: {'ner': 20.22679302333825}\n",
      "Iteration: 58 Loss: {'ner': 18.4314528733315}\n",
      "Iteration: 59 Loss: {'ner': 13.585073264936252}\n",
      "Iteration: 60 Loss: {'ner': 10.596078633279665}\n",
      "Iteration: 61 Loss: {'ner': 27.147928592972782}\n",
      "Iteration: 62 Loss: {'ner': 16.00969394506978}\n",
      "Iteration: 63 Loss: {'ner': 15.261583201981876}\n",
      "Iteration: 64 Loss: {'ner': 4.547370086228068}\n",
      "Iteration: 65 Loss: {'ner': 15.404254418142681}\n",
      "Iteration: 66 Loss: {'ner': 24.453402179764435}\n",
      "Iteration: 67 Loss: {'ner': 35.58755014728159}\n",
      "Iteration: 68 Loss: {'ner': 22.31318458314201}\n",
      "Iteration: 69 Loss: {'ner': 20.712570954161237}\n",
      "Iteration: 70 Loss: {'ner': 19.79818573976103}\n",
      "Iteration: 71 Loss: {'ner': 17.889556758205245}\n",
      "Iteration: 72 Loss: {'ner': 12.443645340611232}\n",
      "Iteration: 73 Loss: {'ner': 11.71984422298191}\n",
      "Iteration: 74 Loss: {'ner': 13.01819464739315}\n",
      "Iteration: 75 Loss: {'ner': 20.26287490988754}\n",
      "Iteration: 76 Loss: {'ner': 13.349545917807582}\n",
      "Iteration: 77 Loss: {'ner': 7.997006070553439}\n",
      "Iteration: 78 Loss: {'ner': 5.057095420427786}\n",
      "Iteration: 79 Loss: {'ner': 14.842375114231253}\n",
      "Iteration: 80 Loss: {'ner': 7.554426969539817}\n",
      "Iteration: 81 Loss: {'ner': 8.151578243437937}\n",
      "Iteration: 82 Loss: {'ner': 13.953117174700953}\n",
      "Iteration: 83 Loss: {'ner': 8.980217876369055}\n",
      "Iteration: 84 Loss: {'ner': 21.35736693415255}\n",
      "Iteration: 85 Loss: {'ner': 21.514176698770804}\n",
      "Iteration: 86 Loss: {'ner': 9.164249802128921}\n",
      "Iteration: 87 Loss: {'ner': 3.891564207623885}\n",
      "Iteration: 88 Loss: {'ner': 10.929220865014155}\n",
      "Iteration: 89 Loss: {'ner': 13.132064599560877}\n",
      "Iteration: 90 Loss: {'ner': 10.879202387742758}\n",
      "Iteration: 91 Loss: {'ner': 14.294051899173}\n",
      "Iteration: 92 Loss: {'ner': 10.261275004393429}\n",
      "Iteration: 93 Loss: {'ner': 7.853898946626196}\n",
      "Iteration: 94 Loss: {'ner': 5.398795698215453}\n",
      "Iteration: 95 Loss: {'ner': 14.42944402340913}\n",
      "Iteration: 96 Loss: {'ner': 36.32645602937412}\n",
      "Iteration: 97 Loss: {'ner': 37.67608894498513}\n",
      "Iteration: 98 Loss: {'ner': 18.5099907920972}\n",
      "Iteration: 99 Loss: {'ner': 15.55657006292686}\n",
      "Iteration: 100 Loss: {'ner': 6.525747457983611}\n"
     ]
    }
   ],
   "source": [
    "data_train = load_json(r'..\\documents\\entidades2-classify.json')\n",
    "\n",
    "data_dict = corrigir_sobreposicao_entidades(data_train)\n",
    "\n",
    "output_dir = r\"..\\documents\"\n",
    "\n",
    "iterations = 100\n",
    "model = train_model(data_dict, iterations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "palavra: disjuntor eletr\n",
      "classe: 12\n",
      "palavra: caix mold\n",
      "classe: 12\n"
     ]
    }
   ],
   "source": [
    "text = \"O disjuntor el\\u00e9trico de caixa moldada \\u00e9 um dispositivo de prote\\u00e7\\u00e3o utilizado em sistemas el\\u00e9tricos de baixa e m\\u00e9dia tens\\u00e3o, cuja fun\\u00e7\\u00e3o \\u00e9 interromper corrente el\\u00e9trica excessiva para evitar danos aos equipamentos.\"\n",
    "text = preprocess(text)\n",
    "\n",
    "doc = model(text)\n",
    "for ent in doc.ents:\n",
    "    print('palavra:',ent.text)\n",
    "    print('classe:',ent.label_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
