{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import random\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_model(data_dict, iterations):\n",
    "    nlp = spacy.blank(\"pt\")\n",
    "\n",
    "    nlp.add_pipe(\"ner\", name=\"ner\", last=True)\n",
    "\n",
    "    for label in data_dict.keys():\n",
    "        nlp.get_pipe(\"ner\").add_label(label)\n",
    "\n",
    "    train_data = []\n",
    "    for label, examples in data_dict.items():\n",
    "        for text, annotations in examples:\n",
    "            train_data.append((text, annotations))\n",
    "\n",
    "    nlp.begin_training()\n",
    "    for itn in range(iterations):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        \n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example_batch = []\n",
    "            for text, annotation in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = spacy.training.example.Example.from_dict(doc, annotation)\n",
    "                example_batch.append(example)\n",
    "            nlp.update(example_batch, losses=losses)\n",
    "        \n",
    "        print(\"Iteration:\", itn+1, \"Loss:\", losses)\n",
    "\n",
    "    return nlp\n",
    "\n",
    "def save_model(model, output_dir):\n",
    "    output_dir = Path(output_dir)\n",
    "    if not output_dir.exists():\n",
    "        output_dir.mkdir()\n",
    "    model_path = output_dir / \"NER_model\"\n",
    "    model.to_disk(model_path)\n",
    "    print(\"Model saved to:\", model_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exemplo de dados dos parâmetros:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = entidades = {\n",
    "    \"BOMBA\": [\n",
    "    (\"Bomba centrífuga\", {\"entities\": [(0, 5, \"BOMBA\")]}),\n",
    "    ]}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preparando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\herik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\herik\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(json_path, content):\n",
    "    with open(json_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(content, f, indent=4)\n",
    "\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_df(file):\n",
    "    df = pd.read_excel(file)\n",
    "    return df\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    keywords = ' '.join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    stems = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return str(stems)\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punct(text)\n",
    "    text = extract_keywords(text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = remove_accent(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "def encontrar_palavras(texto, frases):\n",
    "    resultado = []\n",
    "    \n",
    "    for frase in frases:\n",
    "        padrao = re.compile(r'\\b{}\\b'.format(re.escape(frase)))\n",
    "        correspondencias = padrao.finditer(texto)\n",
    "        \n",
    "        for correspondencia in correspondencias:\n",
    "            dicionario = {\n",
    "                \"text\": frase,\n",
    "                \"startc\": correspondencia.start(),\n",
    "                \"endc\": correspondencia.end() - 1,\n",
    "                \"startp\": len(re.findall(r'\\b\\w+\\b', texto[:correspondencia.start()])),\n",
    "                \"endp\": (len(re.findall(r'\\b\\w+\\b', texto[:correspondencia.start()])) + len(frase.split())) - 1\n",
    "            }\n",
    "            resultado.append(dicionario)\n",
    "    return resultado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_json(r'..\\documents\\entidades2.json')\n",
    "df = load_df('..\\documents\\classes.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = []\n",
    "words = []\n",
    "for idx, row in df.iterrows():\n",
    "    classe = idx + 1\n",
    "    try:\n",
    "        if data[idx]['classe'] == classe:\n",
    "            list_words = row['keywords'].split(',')\n",
    "            for i in range(len(list_words)):\n",
    "                list_words[i] = preprocess(list_words[i]).strip()\n",
    "            words.append(list_words)\n",
    "    except IndexError:\n",
    "        break\n",
    "\n",
    "new_dict = {}\n",
    "for i in range(len(data)):\n",
    "    list_tuple = []\n",
    "    for texto in data[i][\"texts\"]:\n",
    "        texto = preprocess(texto).strip()\n",
    "        list_find_word = encontrar_palavras(texto,words[i])\n",
    "        list_find = []\n",
    "        for j in list_find_word:\n",
    "            list_word_found = [j['startc'],j['endc']+1,str(data[i]['classe'])]\n",
    "            list_find.append(list_word_found)\n",
    "        list_tuple.append((texto, {\"entities\": list_find}))\n",
    "    new_dict[str(data[i]['classe'])] = list_tuple\n",
    "        # dict_ = encontrar_palavras(texto,words)\n",
    "        # new_dict[f'texto{cont}'] = texto\n",
    "        # new_dict['entities'] = dict_\n",
    "        # cont += 1\n",
    "        # new_data.append(new_dict)\n",
    "        # print(new_data)\n",
    "    create_json(r'..\\documents\\entidades3-classify.json',new_dict)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Treinando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def corrigir_sobreposicao_entidades(data_dict):\n",
    "    for label, examples in data_dict.items():\n",
    "        for i in range(len(examples)):\n",
    "            entities = examples[i][1][\"entities\"]\n",
    "            entities_sorted = sorted(entities, key=lambda x: x[1] - x[0], reverse=True)\n",
    "            entities_filtered = []\n",
    "            for j in range(len(entities_sorted)):\n",
    "                entity = entities_sorted[j]\n",
    "                entity_start = entity[0]\n",
    "                entity_end = entity[1]\n",
    "                entity_label = entity[2]\n",
    "                \n",
    "                entity_exists = False\n",
    "                for existing_entity in entities_filtered:\n",
    "                    if (existing_entity[0] <= entity_start <= existing_entity[1]) or (existing_entity[0] <= entity_end <= existing_entity[1]):\n",
    "                        entity_exists = True\n",
    "                        break\n",
    "                \n",
    "                if not entity_exists:\n",
    "                    entities_filtered.append(entity)\n",
    "            \n",
    "            data_dict[label][i][1][\"entities\"] = entities_filtered\n",
    "    \n",
    "    return data_dict\n",
    "\n",
    "def verificar_intervalos(lista):\n",
    "    spans = []\n",
    "    for intervalo_atual in lista:\n",
    "        start, end, label = intervalo_atual\n",
    "        span_atual = (start, end, label)\n",
    "        overlap = False\n",
    "        for span_existente in spans:\n",
    "            if span_existente[0] <= start < span_existente[1] or span_existente[0] < end <= span_existente[1]:\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            spans.append(span_atual)\n",
    "    lista_corrigida = [[start, end, label] for start, end, label in spans]\n",
    "    return lista_corrigida\n",
    "\n",
    "def prepara(dict_):\n",
    "    for i in dict_.values():\n",
    "        for y in i:\n",
    "            for j in y[1].values():\n",
    "                new_value = verificar_intervalos(j)\n",
    "                y[1]['entities'] = new_value\n",
    "    return dict_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1 Loss: {'ner': 1536.3399661156807}\n",
      "Iteration: 2 Loss: {'ner': 419.8025585498161}\n",
      "Iteration: 3 Loss: {'ner': 156.15117192780036}\n",
      "Iteration: 4 Loss: {'ner': 101.82290648591245}\n",
      "Iteration: 5 Loss: {'ner': 115.35646462280877}\n",
      "Iteration: 6 Loss: {'ner': 79.60682365473156}\n",
      "Iteration: 7 Loss: {'ner': 43.26848835395674}\n",
      "Iteration: 8 Loss: {'ner': 41.29418882552955}\n",
      "Iteration: 9 Loss: {'ner': 67.43128388256821}\n",
      "Iteration: 10 Loss: {'ner': 56.58299198088355}\n",
      "Iteration: 11 Loss: {'ner': 35.968468110855575}\n",
      "Iteration: 12 Loss: {'ner': 37.99214003382879}\n",
      "Iteration: 13 Loss: {'ner': 16.801876325791074}\n",
      "Iteration: 14 Loss: {'ner': 28.310037982854542}\n",
      "Iteration: 15 Loss: {'ner': 50.227962798081805}\n",
      "Iteration: 16 Loss: {'ner': 43.660531579426326}\n",
      "Iteration: 17 Loss: {'ner': 38.59656687821507}\n",
      "Iteration: 18 Loss: {'ner': 31.883199426954974}\n",
      "Iteration: 19 Loss: {'ner': 21.192720807780358}\n",
      "Iteration: 20 Loss: {'ner': 16.18484542013092}\n",
      "Iteration: 21 Loss: {'ner': 18.0075120131861}\n",
      "Iteration: 22 Loss: {'ner': 28.151157318548048}\n",
      "Iteration: 23 Loss: {'ner': 64.06577314012785}\n",
      "Iteration: 24 Loss: {'ner': 21.95153512601125}\n",
      "Iteration: 25 Loss: {'ner': 26.057917644448473}\n",
      "Iteration: 26 Loss: {'ner': 50.1831828221528}\n",
      "Iteration: 27 Loss: {'ner': 64.50216230022377}\n",
      "Iteration: 28 Loss: {'ner': 38.16812312224772}\n",
      "Iteration: 29 Loss: {'ner': 26.510553125960904}\n",
      "Iteration: 30 Loss: {'ner': 10.690634827731985}\n",
      "Iteration: 31 Loss: {'ner': 12.96426658829663}\n",
      "Iteration: 32 Loss: {'ner': 5.848815161197723}\n",
      "Iteration: 33 Loss: {'ner': 9.11400047453854}\n",
      "Iteration: 34 Loss: {'ner': 31.9970168357947}\n",
      "Iteration: 35 Loss: {'ner': 10.014467246209717}\n",
      "Iteration: 36 Loss: {'ner': 11.275887299595476}\n",
      "Iteration: 37 Loss: {'ner': 9.079119246296225}\n",
      "Iteration: 38 Loss: {'ner': 26.32306253992194}\n",
      "Iteration: 39 Loss: {'ner': 92.90571440751503}\n",
      "Iteration: 40 Loss: {'ner': 37.620603911025896}\n",
      "Iteration: 41 Loss: {'ner': 30.141783091875514}\n",
      "Iteration: 42 Loss: {'ner': 35.02072965377803}\n",
      "Iteration: 43 Loss: {'ner': 21.55127213575951}\n",
      "Iteration: 44 Loss: {'ner': 16.866077018707646}\n",
      "Iteration: 45 Loss: {'ner': 9.051008658483054}\n",
      "Iteration: 46 Loss: {'ner': 18.240358961260828}\n",
      "Iteration: 47 Loss: {'ner': 7.571323877635846}\n",
      "Iteration: 48 Loss: {'ner': 14.427814855408938}\n",
      "Iteration: 49 Loss: {'ner': 26.78271654431634}\n",
      "Iteration: 50 Loss: {'ner': 20.076293300175955}\n",
      "Iteration: 51 Loss: {'ner': 3.45986766706932}\n",
      "Iteration: 52 Loss: {'ner': 11.444012789815305}\n",
      "Iteration: 53 Loss: {'ner': 9.69823831006425}\n",
      "Iteration: 54 Loss: {'ner': 19.655366368991594}\n",
      "Iteration: 55 Loss: {'ner': 34.86762283552527}\n",
      "Iteration: 56 Loss: {'ner': 28.671742895156356}\n",
      "Iteration: 57 Loss: {'ner': 30.740566301102408}\n",
      "Iteration: 58 Loss: {'ner': 18.4660140139038}\n",
      "Iteration: 59 Loss: {'ner': 10.474898484960022}\n",
      "Iteration: 60 Loss: {'ner': 11.839023898936356}\n",
      "Iteration: 61 Loss: {'ner': 4.139398259477637}\n",
      "Iteration: 62 Loss: {'ner': 13.27719161810451}\n",
      "Iteration: 63 Loss: {'ner': 2.505163093916317}\n",
      "Iteration: 64 Loss: {'ner': 5.195948657983474}\n",
      "Iteration: 65 Loss: {'ner': 10.666654093196364}\n",
      "Iteration: 66 Loss: {'ner': 14.258858431865491}\n",
      "Iteration: 67 Loss: {'ner': 23.994220698476905}\n",
      "Iteration: 68 Loss: {'ner': 21.96834187948697}\n",
      "Iteration: 69 Loss: {'ner': 31.95268967332159}\n",
      "Iteration: 70 Loss: {'ner': 35.05151237032076}\n",
      "Iteration: 71 Loss: {'ner': 19.58202445620695}\n",
      "Iteration: 72 Loss: {'ner': 15.866933247874158}\n",
      "Iteration: 73 Loss: {'ner': 10.638782454365321}\n",
      "Iteration: 74 Loss: {'ner': 14.57969475303252}\n",
      "Iteration: 75 Loss: {'ner': 12.963908933382257}\n",
      "Iteration: 76 Loss: {'ner': 11.501218413349093}\n",
      "Iteration: 77 Loss: {'ner': 15.16310113331952}\n",
      "Iteration: 78 Loss: {'ner': 21.709986232894195}\n",
      "Iteration: 79 Loss: {'ner': 36.01222155618666}\n",
      "Iteration: 80 Loss: {'ner': 21.603415632806446}\n",
      "Iteration: 81 Loss: {'ner': 7.600856079379173}\n",
      "Iteration: 82 Loss: {'ner': 7.9655957319727015}\n",
      "Iteration: 83 Loss: {'ner': 7.481220105217656}\n",
      "Iteration: 84 Loss: {'ner': 3.1588486689825257}\n",
      "Iteration: 85 Loss: {'ner': 4.1663905742279495}\n",
      "Iteration: 86 Loss: {'ner': 5.902292732848194}\n",
      "Iteration: 87 Loss: {'ner': 14.135585635127947}\n",
      "Iteration: 88 Loss: {'ner': 3.5286601032236846}\n",
      "Iteration: 89 Loss: {'ner': 8.528135880464472}\n",
      "Iteration: 90 Loss: {'ner': 17.297913591098567}\n",
      "Iteration: 91 Loss: {'ner': 28.29974303553614}\n",
      "Iteration: 92 Loss: {'ner': 6.404319316853467}\n",
      "Iteration: 93 Loss: {'ner': 11.701631837078423}\n",
      "Iteration: 94 Loss: {'ner': 12.146293779418203}\n",
      "Iteration: 95 Loss: {'ner': 19.648681298116713}\n",
      "Iteration: 96 Loss: {'ner': 16.27770887903202}\n",
      "Iteration: 97 Loss: {'ner': 22.743688147513854}\n",
      "Iteration: 98 Loss: {'ner': 14.988241333964478}\n",
      "Iteration: 99 Loss: {'ner': 11.60524033356257}\n",
      "Iteration: 100 Loss: {'ner': 10.548847415836288}\n",
      "Model saved to: model\\NER_model\n"
     ]
    }
   ],
   "source": [
    "data_train = load_json(r'..\\documents\\entidades3-classify.json')\n",
    "\n",
    "# data_dict = corrigir_sobreposicao_entidades(data_train)\n",
    "data_dict = prepara(data_train)\n",
    "\n",
    "iterations = 100\n",
    "model = train_model(data_dict, iterations)\n",
    "output_dir = r\"model\"\n",
    "save_model(model, output_dir)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classifier(texto):\n",
    "    diretorio_modelo = r'NER_model'\n",
    "    nlp = spacy.load(diretorio_modelo)\n",
    "    doc = nlp(texto)\n",
    "    labels = [entidade.label_ for entidade in doc.ents]\n",
    "    return labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acertou: 472\n",
      "errou: 60\n",
      "precisão: 88.7218045112782\n"
     ]
    }
   ],
   "source": [
    "data_teste = load_json(r'..\\documents\\entidades2_teste.json')\n",
    "\n",
    "acertos = 0\n",
    "erros = 0\n",
    "for i in data_teste:\n",
    "    for j in i['texts']:\n",
    "        text = preprocess(j)\n",
    "        result = classifier(text)\n",
    "        try:\n",
    "            if str(result[0]) == str(i['classe']):\n",
    "                acertos += 1\n",
    "            else:\n",
    "                erros += 1\n",
    "        except IndexError:\n",
    "            erros += 1\n",
    "\n",
    "\n",
    "print('acertou:',acertos)\n",
    "print('errou:',erros)\n",
    "print('precisão:',(acertos/(acertos+erros))*100)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
