{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funções úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "import openai\n",
    "\n",
    "from re import compile, findall, escape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_json(json_path, content):\n",
    "    if os.path.isfile(json_path):\n",
    "        with open(json_path, 'r+', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "            data.append(content)\n",
    "            f.seek(0)\n",
    "            json.dump(data, f, indent=4)\n",
    "    else:\n",
    "        with open(json_path, 'w', encoding='utf-8') as f:\n",
    "            data = [content]\n",
    "            json.dump(data, f, indent=4)\n",
    "\n",
    "def load_json(file):\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    return data\n",
    "\n",
    "def load_df(file):\n",
    "    df = pd.read_excel(file)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    keywords = ' '.join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    stems = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return str(stems)\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punct(text)\n",
    "    text = extract_keywords(text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = remove_accent(text)\n",
    "    return text\n",
    "\n",
    "\n",
    "\n",
    "def chatgpt(txt):\n",
    "  openai.api_key = \"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\"\n",
    "  completion = openai.ChatCompletion.create(\n",
    "    model=\"gpt-3.5-turbo\",\n",
    "    messages=[\n",
    "      {\"role\": \"user\", \"content\": txt}\n",
    "    ]\n",
    "  )\n",
    "  response = completion.choices[0].message.content\n",
    "  return response\n",
    "\n",
    "\n",
    "def find_words(text, find_tokens):\n",
    "    result = []\n",
    "    for token in find_tokens:\n",
    "        pattern = compile(r'\\b{}\\b'.format(escape(token)))\n",
    "        matches = pattern.finditer(text)\n",
    "        for match in matches:\n",
    "            dictionary = {\n",
    "                \"text\": token,\n",
    "                \"start_index\": match.start(),\n",
    "                \"end_index\": match.end() - 1,\n",
    "                \"start_position\": len(findall(r'\\b\\w+\\b', text[:match.start()])),\n",
    "                \"end_position\": (len(findall(r'\\b\\w+\\b', text[:match.start()])) + len(token.split())) - 1\n",
    "            }\n",
    "            result.append(dictionary)\n",
    "    return result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotina de Treino"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotulagem de dados para treino:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_generation(df, column_label, column_keyword, qtd_samples, qtd_values_per_req, num_prompt, path):\n",
    "    for idx, row in df.iterrows():\n",
    "        keywords = row[column_keyword].split(r'.')\n",
    "        new_texts = []\n",
    "        while len(new_texts) < qtd_samples:\n",
    "            for word in keywords:\n",
    "                \n",
    "                prompt_text_tec = f\"faça {qtd_values_per_req} exemplos de textos tecnicos de engenharia/industria sobre o seguinte tema: {word}. retorne no seguinte formato: lista em python, exemplo: ['exemplo1','exemplo2'...]. ps(retorne somente o corpo da lista somente com as strings dos exemplos), não envie textos complementares!\"\n",
    "                prompt_problem = f\"simule ser um cliente de uma empresa de manutenção industrial. faça {qtd_values_per_req} exemplos de textos que descrevem/relatam o seguinte problema: {word}. retorne no seguinte formato: lista em python, exemplo: ['exemplo1','exemplo2'...]. ps(retorne somente o corpo da lista somente com as strings dos exemplos), não envie textos complementares!\"\n",
    "                prompts = [prompt_text_tec, prompt_problem]\n",
    "\n",
    "                retry = True\n",
    "                while retry:\n",
    "                    try:\n",
    "                        response = chatgpt(prompts[num_prompt])\n",
    "                    except:\n",
    "                        time.sleep(60)\n",
    "                        response = chatgpt(prompts[num_prompt])\n",
    "                    try:\n",
    "                        new_texts += eval(response)\n",
    "                        retry = False\n",
    "                    except:\n",
    "                        continue\n",
    "        if len(new_texts) > qtd_samples:\n",
    "            new_texts = random.sample(new_texts, qtd_samples)\n",
    "        dict_ = {'classe':row[column_label], 'texts':new_texts}\n",
    "        create_json(path,dict_)\n",
    "\n",
    "\n",
    "def check_intervals(lst):\n",
    "    spans = []\n",
    "    for current_interval in lst:\n",
    "        start, end, label = current_interval\n",
    "        current_span = (start, end, label)\n",
    "        overlap = False\n",
    "        for existing_span in spans:\n",
    "            if existing_span[0] <= start < existing_span[1] or existing_span[0] < end <= existing_span[1]:\n",
    "                overlap = True\n",
    "                break\n",
    "        if not overlap:\n",
    "            spans.append(current_span)\n",
    "    corrected_list = [[start, end, label] for start, end, label in spans]\n",
    "    return corrected_list\n",
    "\n",
    "def preparing_data(df, column_label, column_keyword, path, labels_with_texts):\n",
    "    words = []\n",
    "    for idx, row in df.iterrows():\n",
    "        list_words = re.split(r',|;|.', row[column_keyword])\n",
    "        for i in range(len(list_words)):\n",
    "            list_words[i] = preprocess(list_words[i]).strip()\n",
    "        words.append(list_words)\n",
    "    dict_train = {}\n",
    "    for i in range(len(labels_with_texts)):\n",
    "        list_tuple = []\n",
    "        for text in labels_with_texts[i][\"texts\"]:\n",
    "            text = preprocess(text).strip()\n",
    "            list_find_word = find_words(text,words[i])\n",
    "            list_find = []\n",
    "            for j in list_find_word:\n",
    "                list_word_found = [j['start_index'],j['end_index']+1,str(labels_with_texts[i][column_label])]\n",
    "                list_find.append(list_word_found)\n",
    "            list_tuple.append((text, {\"entities\": list_find}))\n",
    "        dict_train[str(labels_with_texts[i][column_label])] = list_tuple\n",
    "\n",
    "        for values in dict_train.values():\n",
    "            for items in values:\n",
    "                for inner_values in items[1].values():\n",
    "                    new_value = check_intervals(inner_values)\n",
    "                    items[1]['entities'] = new_value\n",
    "\n",
    "        create_json(path, dict_train)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treinar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download \"pt_core_news_sm\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import minibatch, compounding\n",
    "from spacy import blank, training, load\n",
    "from pathlib import Path\n",
    "nlp = load(\"pt_core_news_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemplo de dado de entrada\n",
    "# data_dict = {\n",
    "#     \"BOMBA\": [\n",
    "#     (\"Bomba centrífuga\", {\"entities\": [(0, 5, \"BOMBA\")]}),\n",
    "#     ]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, path_model):\n",
    "    path_model = Path(path_model)\n",
    "    if not path_model.exists():\n",
    "        path_model.mkdir()\n",
    "    model_path = path_model\n",
    "    model.to_disk(model_path)\n",
    "    print(\"Model saved to:\", model_path)\n",
    "\n",
    "def train_model(data_dict, iterations, path_model):\n",
    "    nlp = blank(\"pt\")\n",
    "    nlp.add_pipe(\"ner\", name=\"ner\", last=True)\n",
    "\n",
    "    for label in data_dict.keys():\n",
    "        nlp.get_pipe(\"ner\").add_label(label)\n",
    "    train_data = []\n",
    "    for label, examples in data_dict.items():\n",
    "        for text, annotations in examples:\n",
    "            train_data.append((text, annotations))\n",
    "    nlp.begin_training()\n",
    "\n",
    "    for itn in range(iterations):\n",
    "        random.shuffle(train_data)\n",
    "        losses = {}\n",
    "        batches = minibatch(train_data, size=compounding(4.0, 32.0, 1.001))\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            example_batch = []\n",
    "            for text, annotation in zip(texts, annotations):\n",
    "                doc = nlp.make_doc(text)\n",
    "                example = training.example.Example.from_dict(doc, annotation)\n",
    "                example_batch.append(example)\n",
    "            nlp.update(example_batch, losses=losses)\n",
    "        print(\"Iteration:\", itn+1, \"Loss:\", losses)\n",
    "\n",
    "    save_model(nlp, path_model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rotina de teste"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rotulagem de dados para teste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_df(r'..\\portugues\\pt_troubleshooting.xlsx')\n",
    "df = df[['problem','equipament']].drop_duplicates()\n",
    "prepared_df = df.applymap(preprocess)\n",
    "data_generation(prepared_df,'equipament','problem',10,5,1,'teste.json')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testar o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import load, displacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">1 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    valvul\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">3</span>\n",
       "</mark>\n",
       " gavet sao ide aplic onde ha necess flux total obstru possu tamb alto desempenh quant vedaça 1 \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    bomb\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">1</span>\n",
       "</mark>\n",
       " \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rolament\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">2</span>\n",
       "</mark>\n",
       " component essenc maquin rotat garant reduça atrit suport carg pod classific divers tip \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rolament\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">2</span>\n",
       "</mark>\n",
       " esfer \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rolament\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">2</span>\n",
       "</mark>\n",
       " rol \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    rolament\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">2</span>\n",
       "</mark>\n",
       " autocompens esfer outr acion \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    corrent\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">4</span>\n",
       "</mark>\n",
       " v sadads \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    filtr ole\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">6</span>\n",
       "</mark>\n",
       " garant funcion \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    motor eletr\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">10</span>\n",
       "</mark>\n",
       "</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def classifier(text, path):\n",
    "    nlp = load(path)\n",
    "    doc = nlp(text)\n",
    "    # labels = [(entidade.text,entidade.label_) for entidade in doc.ents]\n",
    "    labels = displacy.render(doc, style=\"ent\")\n",
    "    return labels\n",
    "\n",
    "text = preprocess(\"1. As valvulas de gaveta sao ideais para aplicações onde ha a necessidade de um fluxo totalmente obstruido. Possuem tambem um alto desempenho quanto a vedaçao. 1. bomba e Os rolamentos são componentes essenciais em maquinas rotativas, garantindo a redução do atrito e o suporte de cargas. Eles podem ser classificados em diversos tipos, como o rolamento de esferas, o rolamento de rolos, o rolamento autocompensador de esferas, entre outros.  Os acionamentos por corrente em v sadadsa de filtro de óleo para garantir o funcionamento de motores elétricos\")\n",
    "a = classifier(text, r\"NER_equipament\")\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
