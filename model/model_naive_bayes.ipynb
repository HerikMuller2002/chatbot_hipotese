{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "import itertools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultinomialNB()\n",
    "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "def naive_bayes(labels, examples):\n",
    "    # Preparação dos dados de treinamento\n",
    "    texts = []\n",
    "    for idx, label in enumerate(labels):\n",
    "        texts.extend(examples[idx])\n",
    "    labels = [label for sublist in examples for label in [labels[examples.index(sublist)]] * len(sublist)]\n",
    "    \n",
    "    global vectorizer\n",
    "    global model\n",
    "\n",
    "    # Criação do vetorizador usando as palavras-chave dos equipamentos\n",
    "    X_train = vectorizer.fit_transform(texts)\n",
    "    # Treinamento do modelo\n",
    "    model.fit(X_train, labels)\n",
    "    \n",
    "    # Fazer previsões nos dados de treinamento\n",
    "    predicted_labels = model.predict(X_train)\n",
    "    \n",
    "    # Calcular a matriz de confusão\n",
    "    confusion_mat = confusion_matrix(labels, predicted_labels)\n",
    "    \n",
    "    # Calcular a acurácia\n",
    "    accuracy = np.trace(confusion_mat) / np.sum(confusion_mat)\n",
    "    print(\"Acurácia: {:.2f}%\".format(accuracy * 100))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from re import sub\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = sub(r'\\d+', '', text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def get_keywords(text):\n",
    "    print(text)\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return keywords\n",
    "\n",
    "def get_synonyms(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    synonyms = []\n",
    "    for word in tokens:\n",
    "        for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "            for lemma in syn.lemmas(lang=\"por\"):\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return stems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando o dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>equipament</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>bombas</td>\n",
       "      <td>bomba</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rolamentos</td>\n",
       "      <td>rolamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>válvulas</td>\n",
       "      <td>válvula</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>acionamentos por corrente</td>\n",
       "      <td>corrente, acionamento corrente</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>caixas de engrenagens</td>\n",
       "      <td>caixa engrenagem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Sistemas de óleo lubrificante</td>\n",
       "      <td>reservatório óleo, bomba óleo, filtro óleo, ra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Acionamentos por correia em V</td>\n",
       "      <td>polias V, correia V, tensionadores V, acoplame...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Sistemas de ventiladores</td>\n",
       "      <td>ventilador, exaustor, soprador</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Purgadores de vapor</td>\n",
       "      <td>Purgadores vapor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Motores elétricos</td>\n",
       "      <td>Motores elétricos</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      equipament   \n",
       "0                         bombas  \\\n",
       "1                     rolamentos   \n",
       "2                       válvulas   \n",
       "3      acionamentos por corrente   \n",
       "4          caixas de engrenagens   \n",
       "5  Sistemas de óleo lubrificante   \n",
       "6  Acionamentos por correia em V   \n",
       "7       Sistemas de ventiladores   \n",
       "8            Purgadores de vapor   \n",
       "9              Motores elétricos   \n",
       "\n",
       "                                            keywords  \n",
       "0                                              bomba  \n",
       "1                                          rolamento  \n",
       "2                                            válvula  \n",
       "3                     corrente, acionamento corrente  \n",
       "4                                   caixa engrenagem  \n",
       "5  reservatório óleo, bomba óleo, filtro óleo, ra...  \n",
       "6  polias V, correia V, tensionadores V, acoplame...  \n",
       "7                     ventilador, exaustor, soprador  \n",
       "8                                   Purgadores vapor  \n",
       "9                                  Motores elétricos  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_excel(r'..\\documents\\classes.xlsx')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "bombas = [\n",
    "    \"Bomba centrífuga\",\n",
    "    \"Bomba de água\",\n",
    "    \"Bomba de vácuo\",\n",
    "    \"Bomba de calor\",\n",
    "    \"Bomba dosadora\",\n",
    "    \"Bomba submersível\",\n",
    "    \"Bomba de pistão\",\n",
    "    \"Bomba de diafragma\",\n",
    "    \"Bomba de engrenagem\",\n",
    "    \"Bomba de incêndio\",\n",
    "    \"Bomba de vácuo rotativa\",\n",
    "    \"Bomba de alta pressão\",\n",
    "    \"Bomba de transferência\",\n",
    "    \"Bomba de ar comprimido\",\n",
    "    \"Bomba de esgoto\",\n",
    "    \"Bomba peristáltica\",\n",
    "    \"Bomba de irrigação\",\n",
    "    \"Bomba de deslocamento positivo\",\n",
    "    \"Bomba de lóbulos\",\n",
    "    \"Bomba de turbina\",\n",
    "    \"Bomba de palhetas\",\n",
    "    \"Bomba de cavidade progressiva\",\n",
    "    \"Bomba hidráulica\",\n",
    "    \"Bomba de duplo diafragma\",\n",
    "    \"Bomba de poço\",\n",
    "    \"Bomba de drenagem\",\n",
    "    \"Bomba de concreto\",\n",
    "    \"Bomba de óleo\",\n",
    "    \"Bomba de químicos\",\n",
    "    \"Bomba de refrigeração\"\n",
    "]\n",
    "rolamentos = [\n",
    "    \"Rolamento de esferas\",\n",
    "    \"Rolamento de rolos\",\n",
    "    \"Rolamento axial\",\n",
    "    \"Rolamento radial\",\n",
    "    \"Rolamento de agulhas\",\n",
    "    \"Rolamento autocompensador\",\n",
    "    \"Rolamento de contato angular\",\n",
    "    \"Rolamento de precisão\",\n",
    "    \"Rolamento magnético\",\n",
    "    \"Rolamento cerâmico\",\n",
    "    \"Rolamento blindado\",\n",
    "    \"Rolamento vedado\",\n",
    "    \"Rolamento linear\",\n",
    "    \"Rolamento de alta temperatura\",\n",
    "    \"Rolamento de baixo atrito\",\n",
    "    \"Rolamento de longa vida útil\",\n",
    "    \"Rolamento de elevada capacidade de carga\",\n",
    "    \"Rolamento de alta velocidade\",\n",
    "    \"Rolamento de rolamento axial de rolos cilíndricos\",\n",
    "    \"Rolamento de rolo cilíndrico\",\n",
    "    \"Rolamento de esferas de contato angular de quatro pontos\",\n",
    "    \"Rolamento de rolo esférico\",\n",
    "    \"Rolamento de giro\",\n",
    "    \"Rolamento para máquinas pesadas\",\n",
    "    \"Rolamento para alta precisão\",\n",
    "    \"Rolamento para altas temperaturas\",\n",
    "    \"Rolamento para baixo ruído\",\n",
    "    \"Rolamento para aplicações industriais\",\n",
    "    \"Rolamento de transmissão de potência\",\n",
    "    \"Rolamento de movimentação linear\"\n",
    "]\n",
    "valvulas = [\n",
    "    \"Válvula de controle\",\n",
    "    \"Válvula de alívio\",\n",
    "    \"Válvula solenóide\",\n",
    "    \"Válvula de esfera\",\n",
    "    \"Válvula de borboleta\",\n",
    "    \"Válvula de retenção\",\n",
    "    \"Válvula de gaveta\",\n",
    "    \"Válvula de diafragma\",\n",
    "    \"Válvula de agulha\",\n",
    "    \"Válvula de segurança\",\n",
    "    \"Válvula de escape\",\n",
    "    \"Válvula de bloqueio\",\n",
    "    \"Válvula reguladora de pressão\",\n",
    "    \"Válvula de descarga\",\n",
    "    \"Válvula de enchimento\",\n",
    "    \"Válvula de descarga a vácuo\",\n",
    "    \"Válvula de reabastecimento\",\n",
    "    \"Válvula de admissão\",\n",
    "    \"Válvula de exaustão\",\n",
    "    \"Válvula de pressão diferencial\",\n",
    "    \"Válvula de controle de vazão\",\n",
    "    \"Válvula de alívio de pressão\",\n",
    "    \"Válvula de retenção de pé\",\n",
    "    \"Válvula de alívio térmico\",\n",
    "    \"Válvula de retenção de disco duplo\",\n",
    "    \"Válvula de controle direcional\",\n",
    "    \"Válvula de retenção de pistão\",\n",
    "    \"Válvula de segurança de pressão\",\n",
    "    \"Válvula de controle proporcional\",\n",
    "    \"Válvula de isolamento\"\n",
    "]\n",
    "acionamentos_corrente = [\n",
    "    \"Acionamento por corrente\",\n",
    "    \"Motor de acionamento por corrente\",\n",
    "    \"Acionamento por corrente elétrica\",\n",
    "    \"Corrente de acionamento\",\n",
    "    \"Sistema de acionamento por corrente\",\n",
    "    \"Acionamento por corrente contínua\",\n",
    "    \"Acionamento por corrente alternada\",\n",
    "    \"Corrente de acionamento elétrico\",\n",
    "    \"Acionamento por corrente trifásica\",\n",
    "    \"Acionamento por corrente monofásica\",\n",
    "    \"Acionamento por corrente direta\",\n",
    "    \"Acionamento por corrente reversa\",\n",
    "    \"Acionamento por corrente pulsada\",\n",
    "    \"Acionamento por corrente de alta tensão\",\n",
    "    \"Acionamento por corrente de baixa tensão\",\n",
    "    \"Acionamento por corrente constante\",\n",
    "    \"Corrente de acionamento ajustável\",\n",
    "    \"Acionamento por corrente variável\",\n",
    "    \"Acionamento por corrente programável\",\n",
    "    \"Corrente de acionamento estável\",\n",
    "    \"Acionamento por corrente de precisão\",\n",
    "    \"Acionamento por corrente de alta potência\",\n",
    "    \"Acionamento por corrente de baixa potência\",\n",
    "    \"Acionamento por corrente de média potência\",\n",
    "    \"Corrente de acionamento industrial\",\n",
    "    \"Acionamento por corrente de tração\",\n",
    "    \"Acionamento por corrente de transporte\",\n",
    "    \"Acionamento por corrente de elevação\",\n",
    "    \"Corrente de acionamento automático\",\n",
    "    \"Acionamento por corrente de controle\",\n",
    "]\n",
    "caixas_engrenagens = [\n",
    "    \"Caixa de engrenagens\",\n",
    "    \"Engrenagens industriais\",\n",
    "    \"Sistema de transmissão por engrenagens\",\n",
    "    \"Caixa de engrenagens helicoidais\",\n",
    "    \"Caixa de engrenagens cônicas\",\n",
    "    \"Engrenagens de alta precisão\",\n",
    "    \"Caixa de engrenagens de redução\",\n",
    "    \"Engrenagens de grande torque\",\n",
    "    \"Caixa de engrenagens de alta potência\",\n",
    "    \"Engrenagens de baixo ruído\",\n",
    "    \"Caixa de engrenagens de velocidade variável\",\n",
    "    \"Engrenagens de transmissão\",\n",
    "    \"Caixa de engrenagens de eixos paralelos\",\n",
    "    \"Engrenagens de dentes retos\",\n",
    "    \"Caixa de engrenagens planetárias\",\n",
    "    \"Engrenagens de eixos cruzados\",\n",
    "    \"Caixa de engrenagens de precisão\",\n",
    "    \"Engrenagens de múltiplas velocidades\",\n",
    "    \"Caixa de engrenagens de montagem fácil\",\n",
    "    \"Engrenagens de longa vida útil\",\n",
    "    \"Caixa de engrenagens de operação suave\",\n",
    "    \"Engrenagens de alto desempenho\",\n",
    "    \"Caixa de engrenagens de serviço pesado\",\n",
    "    \"Engrenagens de reversão\",\n",
    "    \"Caixa de engrenagens de transmissão de potência\",\n",
    "    \"Engrenagens de lubrificação permanente\",\n",
    "    \"Caixa de engrenagens de carga elevada\",\n",
    "    \"Engrenagens de troca rápida\",\n",
    "    \"Caixa de engrenagens de operação silenciosa\",\n",
    "    \"Engrenagens de baixa manutenção\",\n",
    "]\n",
    "sistemas_oleo_lubrificante = [\n",
    "    \"Sistema de óleo lubrificante\",\n",
    "    \"Bomba de óleo\",\n",
    "    \"Filtro de óleo\",\n",
    "    \"Radiador de óleo\",\n",
    "    \"Trocador de óleo\",\n",
    "    \"Válvula de óleo\",\n",
    "    \"Sensor de óleo\",\n",
    "    \"Tubulação de óleo\",\n",
    "    \"Respiro de óleo\",\n",
    "    \"Reservatório de óleo\",\n",
    "    \"Óleo lubrificante\",\n",
    "    \"Sistema de lubrificação\",\n",
    "    \"Fluido lubrificante\",\n",
    "    \"Viscosidade do óleo\",\n",
    "    \"Análise de óleo\",\n",
    "    \"Nível de óleo\",\n",
    "    \"Troca de óleo\",\n",
    "    \"Contaminação do óleo\",\n",
    "    \"Viscosímetro de óleo\",\n",
    "    \"Bomba de circulação de óleo\",\n",
    "    \"Monitoramento do óleo\",\n",
    "    \"Tratamento do óleo\",\n",
    "    \"Descarte de óleo\",\n",
    "    \"Amostragem de óleo\",\n",
    "    \"Sistema de filtragem de óleo\",\n",
    "    \"Bomba de transferência de óleo\",\n",
    "    \"Válvula de alívio de pressão do óleo\",\n",
    "    \"Fluxo de óleo\",\n",
    "    \"Sistema de resfriamento de óleo\",\n",
    "    \"Sistema de aquecimento de óleo\",\n",
    "]\n",
    "acionamentos_correia_v = [\n",
    "    \"Acionamentos por correia em V\",\n",
    "    \"Polias em V\",\n",
    "    \"Correias em V\",\n",
    "    \"Tensionadores em V\",\n",
    "    \"Acoplamentos em V\",\n",
    "    \"Esticadores em V\",\n",
    "    \"Redutores em V\",\n",
    "    \"Suportes em V\",\n",
    "    \"Sistemas de transmissão em V\",\n",
    "    \"Mecanismos de acionamento em V\",\n",
    "    \"Polias e correias em V\",\n",
    "    \"Sincronização em V\",\n",
    "    \"Ajuste de correias em V\",\n",
    "    \"Troca de correias em V\",\n",
    "    \"Manutenção de acionamentos em V\",\n",
    "    \"Desgaste de correias em V\",\n",
    "    \"Alinhamento de correias em V\",\n",
    "    \"Tensão das correias em V\",\n",
    "    \"Substituição de correias em V\",\n",
    "    \"Monitoramento de acionamentos em V\",\n",
    "    \"Vida útil das correias em V\",\n",
    "    \"Eficiência dos acionamentos em V\",\n",
    "    \"Transmissão de potência em V\",\n",
    "    \"Ruído nos acionamentos em V\",\n",
    "    \"Vibração nos acionamentos em V\",\n",
    "    \"Inspeção dos acionamentos em V\",\n",
    "    \"Lubrificação dos acionamentos em V\",\n",
    "    \"Falha nas correias em V\",\n",
    "    \"Desalinhamento dos acionamentos em V\",\n",
    "    \"Desbalanceamento dos acionamentos em V\",\n",
    "]\n",
    "sistemas_ventiladores = [\n",
    "    \"Sistemas de ventiladores\",\n",
    "    \"Ventiladores industriais\",\n",
    "    \"Exaustores\",\n",
    "    \"Sopradores\",\n",
    "    \"Ventilação industrial\",\n",
    "    \"Manutenção de ventiladores\",\n",
    "    \"Limpeza de ventiladores\",\n",
    "    \"Inspeção de ventiladores\",\n",
    "    \"Eficiência dos ventiladores\",\n",
    "    \"Controle de velocidade dos ventiladores\",\n",
    "    \"Ruído dos ventiladores\",\n",
    "    \"Vibração dos ventiladores\",\n",
    "    \"Monitoramento dos ventiladores\",\n",
    "    \"Fluxo de ar dos ventiladores\",\n",
    "    \"Pressão dos ventiladores\",\n",
    "    \"Temperatura dos ventiladores\",\n",
    "    \"Motor dos ventiladores\",\n",
    "    \"Filtragem do ar dos ventiladores\",\n",
    "    \"Troca de filtros dos ventiladores\",\n",
    "    \"Dutos de ar dos ventiladores\",\n",
    "    \"Sistema de exaustão\",\n",
    "    \"Sistema de insuflamento de ar\",\n",
    "    \"Sistema de ventilação industrial\",\n",
    "    \"Seleção de ventiladores\",\n",
    "    \"Dimensionamento de ventiladores\",\n",
    "    \"Curvas de desempenho dos ventiladores\",\n",
    "    \"Mancais dos ventiladores\",\n",
    "    \"Lubrificação dos ventiladores\",\n",
    "    \"Falha nos ventiladores\",\n",
    "    \"Substituição de ventiladores\"\n",
    "]\n",
    "purgadores_vapor = [\n",
    "    \"Purgadores de vapor\",\n",
    "    \"Sistema de purga de vapor\",\n",
    "    \"Condensado de vapor\",\n",
    "    \"Descarga de condensado\",\n",
    "    \"Válvulas de purga de vapor\",\n",
    "    \"Dreno de vapor\",\n",
    "    \"Manutenção de purgadores de vapor\",\n",
    "    \"Limpeza de purgadores de vapor\",\n",
    "    \"Inspeção de purgadores de vapor\",\n",
    "    \"Eficiência dos purgadores de vapor\",\n",
    "    \"Perda de vapor\",\n",
    "    \"Vazamento de vapor\",\n",
    "    \"Monitoramento de purgadores de vapor\",\n",
    "    \"Temperatura do condensado de vapor\",\n",
    "    \"Pressão do condensado de vapor\",\n",
    "    \"Nível de condensado de vapor\",\n",
    "    \"Fluxo de condensado de vapor\",\n",
    "    \"Troca de purgadores de vapor\",\n",
    "    \"Sistema de retorno de condensado\",\n",
    "    \"Sistema de distribuição de vapor\",\n",
    "    \"Sistema de condensação de vapor\",\n",
    "    \"Dimensionamento de purgadores de vapor\",\n",
    "    \"Seleção de purgadores de vapor\",\n",
    "    \"Curvas de desempenho de purgadores de vapor\",\n",
    "    \"Mancais de purgadores de vapor\",\n",
    "    \"Lubrificação de purgadores de vapor\",\n",
    "    \"Falha nos purgadores de vapor\",\n",
    "    \"Substituição de purgadores de vapor\",\n",
    "    \"Recuperação de energia de vapor\",\n",
    "    \"Tratamento de condensado de vapor\"\n",
    "]\n",
    "motores_eletricos = [\n",
    "    \"Motores elétricos\",\n",
    "    \"Motor de indução\",\n",
    "    \"Motor de corrente alternada\",\n",
    "    \"Motor de corrente contínua\",\n",
    "    \"Motor trifásico\",\n",
    "    \"Motor monofásico\",\n",
    "    \"Motor assíncrono\",\n",
    "    \"Motor síncrono\",\n",
    "    \"Partida de motores\",\n",
    "    \"Parada de motores\",\n",
    "    \"Manutenção de motores elétricos\",\n",
    "    \"Limpeza de motores elétricos\",\n",
    "    \"Inspeção de motores elétricos\",\n",
    "    \"Eficiência dos motores elétricos\",\n",
    "    \"Monitoramento de motores elétricos\",\n",
    "    \"Ruído dos motores elétricos\",\n",
    "    \"Vibração dos motores elétricos\",\n",
    "    \"Temperatura dos motores elétricos\",\n",
    "    \"Corrente dos motores elétricos\",\n",
    "    \"Tensão dos motores elétricos\",\n",
    "    \"Potência dos motores elétricos\",\n",
    "    \"Enrolamentos dos motores elétricos\",\n",
    "    \"Isolamento dos motores elétricos\",\n",
    "    \"Falha nos motores elétricos\",\n",
    "    \"Substituição de motores elétricos\",\n",
    "    \"Rebobinamento de motores elétricos\",\n",
    "    \"Sobrecarga de motores elétricos\",\n",
    "    \"Proteção de motores elétricos\",\n",
    "    \"Eficiência energética dos motores elétricos\",\n",
    "    \"Vida útil dos motores elétricos\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bomba centrífuga\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Semeq/nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Semeq\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(words)):\n\u001b[0;32m      3\u001b[0m     \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(words[i])):\n\u001b[1;32m----> 4\u001b[0m         x \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(get_keywords(remove_num(words[i][j])))\n\u001b[0;32m      5\u001b[0m         words[i][j] \u001b[39m=\u001b[39m remove_accent(preprocess_stem(x))\u001b[39m.\u001b[39mlower()\n",
      "Cell \u001b[1;32mIn[23], line 13\u001b[0m, in \u001b[0;36mget_keywords\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_keywords\u001b[39m(text):\n\u001b[0;32m     12\u001b[0m     \u001b[39mprint\u001b[39m(text)\n\u001b[1;32m---> 13\u001b[0m     tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     14\u001b[0m     keywords \u001b[39m=\u001b[39m []\n\u001b[0;32m     15\u001b[0m     \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m tokens:\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Semeq/nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Semeq\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "words = [bombas,rolamentos,valvulas,acionamentos_corrente,caixas_engrenagens,sistemas_oleo_lubrificante,acionamentos_correia_v,sistemas_ventiladores,purgadores_vapor,motores_eletricos]\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words[i])):\n",
    "        x = ' '.join(get_keywords(remove_num(words[i][j])))\n",
    "        words[i][j] = remove_accent(preprocess_stem(x)).lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Semeq/nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Semeq\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m dict_ \u001b[39m=\u001b[39m {}\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m idx, row \u001b[39min\u001b[39;00m df\u001b[39m.\u001b[39miterrows():\n\u001b[1;32m----> 3\u001b[0m     classe \u001b[39m=\u001b[39m  remove_accent(preprocess_stem(remove_num(row[\u001b[39m'\u001b[39;49m\u001b[39mequipament\u001b[39;49m\u001b[39m'\u001b[39;49m])))\u001b[39m.\u001b[39mlower()\n\u001b[0;32m      4\u001b[0m     keywords \u001b[39m=\u001b[39m remove_accent(preprocess_stem(remove_num(row[\u001b[39m'\u001b[39m\u001b[39mkeywords\u001b[39m\u001b[39m'\u001b[39m])))\u001b[39m.\u001b[39mlower()\n\u001b[0;32m      5\u001b[0m     dict_[classe] \u001b[39m=\u001b[39m keywords\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m,\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn[23], line 46\u001b[0m, in \u001b[0;36mpreprocess_stem\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpreprocess_stem\u001b[39m(text):\n\u001b[0;32m     45\u001b[0m     stemmer \u001b[39m=\u001b[39m SnowballStemmer(\u001b[39m\"\u001b[39m\u001b[39mportuguese\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 46\u001b[0m     tokens \u001b[39m=\u001b[39m word_tokenize(text)\n\u001b[0;32m     47\u001b[0m     stems \u001b[39m=\u001b[39m []\n\u001b[0;32m     48\u001b[0m     \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m tokens:\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:129\u001b[0m, in \u001b[0;36mword_tokenize\u001b[1;34m(text, language, preserve_line)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mword_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m, preserve_line\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[0;32m    115\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    116\u001b[0m \u001b[39m    Return a tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m    117\u001b[0m \u001b[39m    using NLTK's recommended word tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m    :type preserve_line: bool\u001b[39;00m\n\u001b[0;32m    128\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 129\u001b[0m     sentences \u001b[39m=\u001b[39m [text] \u001b[39mif\u001b[39;00m preserve_line \u001b[39melse\u001b[39;00m sent_tokenize(text, language)\n\u001b[0;32m    130\u001b[0m     \u001b[39mreturn\u001b[39;00m [\n\u001b[0;32m    131\u001b[0m         token \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences \u001b[39mfor\u001b[39;00m token \u001b[39min\u001b[39;00m _treebank_word_tokenizer\u001b[39m.\u001b[39mtokenize(sent)\n\u001b[0;32m    132\u001b[0m     ]\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\tokenize\\__init__.py:106\u001b[0m, in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39msent_tokenize\u001b[39m(text, language\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39menglish\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m     97\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m     98\u001b[0m \u001b[39m    Return a sentence-tokenized copy of *text*,\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[39m    using NLTK's recommended sentence tokenizer\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39m    :param language: the model name in the Punkt corpus\u001b[39;00m\n\u001b[0;32m    105\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     tokenizer \u001b[39m=\u001b[39m load(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mtokenizers/punkt/\u001b[39;49m\u001b[39m{\u001b[39;49;00mlanguage\u001b[39m}\u001b[39;49;00m\u001b[39m.pickle\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    107\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer\u001b[39m.\u001b[39mtokenize(text)\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\data.py:750\u001b[0m, in \u001b[0;36mload\u001b[1;34m(resource_url, format, cache, verbose, logic_parser, fstruct_reader, encoding)\u001b[0m\n\u001b[0;32m    747\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m<<Loading \u001b[39m\u001b[39m{\u001b[39;00mresource_url\u001b[39m}\u001b[39;00m\u001b[39m>>\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    749\u001b[0m \u001b[39m# Load the resource.\u001b[39;00m\n\u001b[1;32m--> 750\u001b[0m opened_resource \u001b[39m=\u001b[39m _open(resource_url)\n\u001b[0;32m    752\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mformat\u001b[39m \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    753\u001b[0m     resource_val \u001b[39m=\u001b[39m opened_resource\u001b[39m.\u001b[39mread()\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\data.py:876\u001b[0m, in \u001b[0;36m_open\u001b[1;34m(resource_url)\u001b[0m\n\u001b[0;32m    873\u001b[0m protocol, path_ \u001b[39m=\u001b[39m split_resource_url(resource_url)\n\u001b[0;32m    875\u001b[0m \u001b[39mif\u001b[39;00m protocol \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnltk\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[1;32m--> 876\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, path \u001b[39m+\u001b[39;49m [\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m])\u001b[39m.\u001b[39mopen()\n\u001b[0;32m    877\u001b[0m \u001b[39melif\u001b[39;00m protocol\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mfile\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    878\u001b[0m     \u001b[39m# urllib might not use mode='rb', so handle this one ourselves:\u001b[39;00m\n\u001b[0;32m    879\u001b[0m     \u001b[39mreturn\u001b[39;00m find(path_, [\u001b[39m\"\u001b[39m\u001b[39m\"\u001b[39m])\u001b[39m.\u001b[39mopen()\n",
      "File \u001b[1;32mc:\\Users\\Semeq\\Desktop\\chats\\chatbot_hipotese\\.venv\\lib\\site-packages\\nltk\\data.py:583\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    581\u001b[0m sep \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m*\u001b[39m\u001b[39m\"\u001b[39m \u001b[39m*\u001b[39m \u001b[39m70\u001b[39m\n\u001b[0;32m    582\u001b[0m resource_not_found \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00mmsg\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m{\u001b[39;00msep\u001b[39m}\u001b[39;00m\u001b[39m\\n\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 583\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt/english.pickle\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Semeq/nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Semeq\\\\Desktop\\\\chats\\\\chatbot_hipotese\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Semeq\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n    - ''\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "dict_ = {}\n",
    "for idx, row in df.iterrows():\n",
    "    classe =  remove_accent(preprocess_stem(remove_num(row['equipament']))).lower()\n",
    "    keywords = remove_accent(preprocess_stem(remove_num(row['keywords']))).lower()\n",
    "    dict_[classe] = keywords.split(',')\n",
    "lista_classes = [i[0] for i in list(enumerate(dict_.keys()))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_teste = pd.read_excel(r'..\\documents\\teste_modelo.xlsx')\n",
    "df_teste.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(bombas))\n",
    "for idx, row in df_teste.iterrows():\n",
    "    if row['classe'] == 0:\n",
    "        bombas.append(row['exemplos'])\n",
    "    elif row['classe'] == 1:\n",
    "        rolamentos.append(row['exemplos'])\n",
    "    elif row['classe'] == 2:\n",
    "        valvulas.append(row['exemplos'])\n",
    "    elif row['classe'] == 3:\n",
    "        acionamentos_corrente.append(row['exemplos'])\n",
    "    elif row['classe'] == 4:\n",
    "        caixas_engrenagens.append(row['exemplos'])\n",
    "    elif row['classe'] == 5:\n",
    "        sistemas_oleo_lubrificante.append(row['exemplos'])\n",
    "    elif row['classe'] == 6:\n",
    "        acionamentos_correia_v.append(row['exemplos'])\n",
    "    elif row['classe'] == 7:\n",
    "        sistemas_ventiladores.append(row['exemplos'])\n",
    "    elif row['classe'] == 8:\n",
    "        purgadores_vapor.append(row['exemplos'])\n",
    "    elif row['classe'] == 9:\n",
    "        motores_eletricos.append(row['exemplos'])\n",
    "print(len(bombas))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_embaralhado = df_teste.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_word(word):\n",
    "    global vectorizer\n",
    "    global model\n",
    "    \n",
    "    # Vetorizar a palavra usando o vetorizador global\n",
    "    X_test = vectorizer.transform([word])\n",
    "    \n",
    "    # Fazer a previsão usando o modelo treinado\n",
    "    predicted_label = model.predict(X_test)[0]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "# Chamar a função naive_bayes para treinar o modelo e ajustar o vetorizador\n",
    "naive_bayes(lista_classes, words)\n",
    "\n",
    "acertos = 0\n",
    "erros = 0\n",
    "for idx, row in df_teste.iterrows():\n",
    "    word = row['exemplos']\n",
    "    classe = row['classe']\n",
    "    word = ' '.join(get_keywords(remove_num(word)))\n",
    "    word = remove_accent(preprocess_stem(remove_punct(word))).lower()\n",
    "    predicted_label = classify_word(word)\n",
    "    print(\"Palavra:\", word)\n",
    "    print(\"Classe:\", classe)\n",
    "    print(\"Classificação prevista:\", predicted_label)\n",
    "    print()\n",
    "    if predicted_label == classe:\n",
    "        acertos += 1\n",
    "    else:\n",
    "        erros += 1\n",
    "print('acertos:', acertos)\n",
    "print('erros:', erros)\n",
    "print(acertos * 100/(acertos+erros),r'% de precisão')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def similares(frase, lista_classes):\n",
    "    palavras_similares = []\n",
    "    palavras_frase = word_tokenize(frase.lower())\n",
    "\n",
    "    for palavra in palavras_frase:\n",
    "        similaridades = []\n",
    "        for classe in lista_classes:\n",
    "            similaridade = fuzz.ratio(palavra, classe)\n",
    "            similaridades.append((classe, similaridade))\n",
    "\n",
    "        similaridades = sorted(similaridades, key=lambda x: x[1], reverse=True)\n",
    "        if similaridades and similaridades[0][1] >= 80:\n",
    "            palavra_similar = {\n",
    "                \"palavra\": palavra,\n",
    "                \"classe\": similaridades[0][0],\n",
    "                \"similaridade\": similaridades[0][1]\n",
    "            }\n",
    "        else:\n",
    "            palavra_similar = {\n",
    "                \"palavra\": palavra,\n",
    "                \"classe\": None,\n",
    "                \"similaridade\": 0\n",
    "            }\n",
    "        \n",
    "        palavras_similares.append(palavra_similar)\n",
    "\n",
    "    return palavras_similares\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
