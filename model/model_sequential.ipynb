{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from pickle import dump, load\n",
    "from numpy import array\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Semeq\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    keywords = ' '.join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    stems = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return str(stems)\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punct(text)\n",
    "    text = extract_keywords(text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = remove_accent(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_documents(list_obj):\n",
    "    words = []\n",
    "    documents = []\n",
    "    for obj in list_obj:\n",
    "        label = obj['label']\n",
    "        samples = obj['keywords']\n",
    "        for sample in samples:\n",
    "            words.extend(sample)\n",
    "            documents.append((sample, label))\n",
    "    return words, documents\n",
    "\n",
    "def train_model1(list_obj):\n",
    "    labels = []\n",
    "    for obj in list_obj:\n",
    "        labels.append(obj['label'])\n",
    "\n",
    "    words, documents = preparing_documents(list_obj)\n",
    "\n",
    "    words = sorted(list(set(words)))\n",
    "    labels = sorted(list(set(labels)))\n",
    "\n",
    "    words_path = \"words.pkl\"\n",
    "    labels_path = \"labels.pkl\"\n",
    "\n",
    "    dump(words, open(words_path, 'wb'))\n",
    "    dump(labels, open(labels_path, 'wb'))\n",
    "\n",
    "    training = []\n",
    "    output_empty = [0] * len(labels)\n",
    "    for document in documents:\n",
    "        bag = []\n",
    "        pattern_words = document[0]\n",
    "        for word in words:\n",
    "            bag.append(1) if word in pattern_words else bag.append(0)\n",
    "        while len(bag) < len(words):\n",
    "            bag.append(0)\n",
    "        output_row = list(output_empty)\n",
    "        output_row[labels.index(document[1])] = 1\n",
    "        training.append([bag, output_row])\n",
    "\n",
    "    shuffle(training)\n",
    "    training = array(training, dtype=object)\n",
    "\n",
    "    x = list(training[:, 0])\n",
    "    y = list(training[:, 1])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(x[0]),), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    m = model.fit(array(x), array(y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "    model_path = (\"model.h5\")\n",
    "    model.save(model_path, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Semeq\\Desktop\\chatbot_hipotese\\.venv\\lib\\site-packages\\keras\\optimizers\\legacy\\gradient_descent.py:114: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super().__init__(name, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "220/220 [==============================] - 1s 2ms/step - loss: 3.1055 - accuracy: 0.0664\n",
      "Epoch 2/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.9490 - accuracy: 0.0855\n",
      "Epoch 3/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.7859 - accuracy: 0.1318\n",
      "Epoch 4/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.6877 - accuracy: 0.1436\n",
      "Epoch 5/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.6056 - accuracy: 0.1591\n",
      "Epoch 6/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.5862 - accuracy: 0.1809\n",
      "Epoch 7/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.5539 - accuracy: 0.2009\n",
      "Epoch 8/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.5348 - accuracy: 0.1773\n",
      "Epoch 9/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.5251 - accuracy: 0.2127\n",
      "Epoch 10/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.5029 - accuracy: 0.1991\n",
      "Epoch 11/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.4432 - accuracy: 0.2391\n",
      "Epoch 12/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.4809 - accuracy: 0.2282\n",
      "Epoch 13/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3841 - accuracy: 0.2418\n",
      "Epoch 14/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3818 - accuracy: 0.2318\n",
      "Epoch 15/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3763 - accuracy: 0.2336\n",
      "Epoch 16/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3749 - accuracy: 0.2473\n",
      "Epoch 17/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3926 - accuracy: 0.2182\n",
      "Epoch 18/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3910 - accuracy: 0.2473\n",
      "Epoch 19/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3830 - accuracy: 0.2482\n",
      "Epoch 20/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3615 - accuracy: 0.2473\n",
      "Epoch 21/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3330 - accuracy: 0.2727\n",
      "Epoch 22/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3550 - accuracy: 0.2545\n",
      "Epoch 23/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3462 - accuracy: 0.2591\n",
      "Epoch 24/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3439 - accuracy: 0.2364\n",
      "Epoch 25/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3027 - accuracy: 0.2673\n",
      "Epoch 26/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3352 - accuracy: 0.2700\n",
      "Epoch 27/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3342 - accuracy: 0.2636\n",
      "Epoch 28/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2263 - accuracy: 0.2827\n",
      "Epoch 29/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2995 - accuracy: 0.2755\n",
      "Epoch 30/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2871 - accuracy: 0.2536\n",
      "Epoch 31/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2782 - accuracy: 0.2655\n",
      "Epoch 32/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2409 - accuracy: 0.2745\n",
      "Epoch 33/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2828 - accuracy: 0.2591\n",
      "Epoch 34/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2930 - accuracy: 0.2600\n",
      "Epoch 35/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2391 - accuracy: 0.2773\n",
      "Epoch 36/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2658 - accuracy: 0.2636\n",
      "Epoch 37/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2747 - accuracy: 0.2600\n",
      "Epoch 38/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2488 - accuracy: 0.2836\n",
      "Epoch 39/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2413 - accuracy: 0.3073\n",
      "Epoch 40/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1961 - accuracy: 0.2936\n",
      "Epoch 41/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2445 - accuracy: 0.2791\n",
      "Epoch 42/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2469 - accuracy: 0.2782\n",
      "Epoch 43/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2448 - accuracy: 0.2836\n",
      "Epoch 44/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2325 - accuracy: 0.2736\n",
      "Epoch 45/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2640 - accuracy: 0.2782\n",
      "Epoch 46/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2452 - accuracy: 0.2864\n",
      "Epoch 47/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2242 - accuracy: 0.2864\n",
      "Epoch 48/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2387 - accuracy: 0.2791\n",
      "Epoch 49/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2270 - accuracy: 0.2873\n",
      "Epoch 50/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2483 - accuracy: 0.3018\n",
      "Epoch 51/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3202 - accuracy: 0.2864\n",
      "Epoch 52/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2386 - accuracy: 0.2900\n",
      "Epoch 53/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.1983 - accuracy: 0.2945\n",
      "Epoch 54/200\n",
      "220/220 [==============================] - 0s 1ms/step - loss: 2.2468 - accuracy: 0.3045\n",
      "Epoch 55/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2125 - accuracy: 0.2882\n",
      "Epoch 56/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2761 - accuracy: 0.2791\n",
      "Epoch 57/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1708 - accuracy: 0.3082\n",
      "Epoch 58/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2141 - accuracy: 0.3000\n",
      "Epoch 59/200\n",
      "220/220 [==============================] - 1s 2ms/step - loss: 2.2978 - accuracy: 0.2809\n",
      "Epoch 60/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.3222 - accuracy: 0.2927\n",
      "Epoch 61/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2763 - accuracy: 0.3045\n",
      "Epoch 62/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1597 - accuracy: 0.3009\n",
      "Epoch 63/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2952 - accuracy: 0.2891\n",
      "Epoch 64/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2471 - accuracy: 0.2864\n",
      "Epoch 65/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2559 - accuracy: 0.2873\n",
      "Epoch 66/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2433 - accuracy: 0.2782\n",
      "Epoch 67/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1840 - accuracy: 0.3100\n",
      "Epoch 68/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2012 - accuracy: 0.3118\n",
      "Epoch 69/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2155 - accuracy: 0.2964\n",
      "Epoch 70/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2298 - accuracy: 0.2973\n",
      "Epoch 71/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2040 - accuracy: 0.2909\n",
      "Epoch 72/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2208 - accuracy: 0.3045\n",
      "Epoch 73/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2354 - accuracy: 0.3045\n",
      "Epoch 74/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1999 - accuracy: 0.2936\n",
      "Epoch 75/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2227 - accuracy: 0.2882\n",
      "Epoch 76/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1974 - accuracy: 0.2918\n",
      "Epoch 77/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1376 - accuracy: 0.3182\n",
      "Epoch 78/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1804 - accuracy: 0.3091\n",
      "Epoch 79/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1397 - accuracy: 0.3082\n",
      "Epoch 80/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1677 - accuracy: 0.3155\n",
      "Epoch 81/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2334 - accuracy: 0.2891\n",
      "Epoch 82/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1858 - accuracy: 0.3018\n",
      "Epoch 83/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2157 - accuracy: 0.2982\n",
      "Epoch 84/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2037 - accuracy: 0.3073\n",
      "Epoch 85/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2502 - accuracy: 0.2900\n",
      "Epoch 86/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2319 - accuracy: 0.2927\n",
      "Epoch 87/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1742 - accuracy: 0.3191\n",
      "Epoch 88/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2212 - accuracy: 0.3118\n",
      "Epoch 89/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1952 - accuracy: 0.2900\n",
      "Epoch 90/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2128 - accuracy: 0.2955\n",
      "Epoch 91/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2562 - accuracy: 0.3036\n",
      "Epoch 92/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1885 - accuracy: 0.3136\n",
      "Epoch 93/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2213 - accuracy: 0.3073\n",
      "Epoch 94/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1508 - accuracy: 0.3127\n",
      "Epoch 95/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1397 - accuracy: 0.3109\n",
      "Epoch 96/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1393 - accuracy: 0.3245\n",
      "Epoch 97/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1201 - accuracy: 0.3064\n",
      "Epoch 98/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2704 - accuracy: 0.3000\n",
      "Epoch 99/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1866 - accuracy: 0.3027\n",
      "Epoch 100/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1528 - accuracy: 0.3291\n",
      "Epoch 101/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2156 - accuracy: 0.2982\n",
      "Epoch 102/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0955 - accuracy: 0.3318\n",
      "Epoch 103/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1906 - accuracy: 0.3273\n",
      "Epoch 104/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1530 - accuracy: 0.3091\n",
      "Epoch 105/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1545 - accuracy: 0.3264\n",
      "Epoch 106/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1511 - accuracy: 0.3173\n",
      "Epoch 107/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1885 - accuracy: 0.3091\n",
      "Epoch 108/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0938 - accuracy: 0.3327\n",
      "Epoch 109/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1745 - accuracy: 0.3264\n",
      "Epoch 110/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1416 - accuracy: 0.3091\n",
      "Epoch 111/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1914 - accuracy: 0.3018\n",
      "Epoch 112/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1428 - accuracy: 0.3309\n",
      "Epoch 113/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1616 - accuracy: 0.3109\n",
      "Epoch 114/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1579 - accuracy: 0.3109\n",
      "Epoch 115/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1115 - accuracy: 0.3409\n",
      "Epoch 116/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1484 - accuracy: 0.3264\n",
      "Epoch 117/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0942 - accuracy: 0.3318\n",
      "Epoch 118/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1797 - accuracy: 0.2927\n",
      "Epoch 119/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1343 - accuracy: 0.3318\n",
      "Epoch 120/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0890 - accuracy: 0.3509\n",
      "Epoch 121/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1995 - accuracy: 0.3291\n",
      "Epoch 122/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1488 - accuracy: 0.3364\n",
      "Epoch 123/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1309 - accuracy: 0.3209\n",
      "Epoch 124/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2070 - accuracy: 0.3109\n",
      "Epoch 125/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1708 - accuracy: 0.3336\n",
      "Epoch 126/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1727 - accuracy: 0.3136\n",
      "Epoch 127/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1110 - accuracy: 0.3273\n",
      "Epoch 128/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1876 - accuracy: 0.3245\n",
      "Epoch 129/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1414 - accuracy: 0.3091\n",
      "Epoch 130/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1724 - accuracy: 0.2945\n",
      "Epoch 131/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1591 - accuracy: 0.3255\n",
      "Epoch 132/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0995 - accuracy: 0.3409\n",
      "Epoch 133/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1872 - accuracy: 0.3209\n",
      "Epoch 134/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1308 - accuracy: 0.3309\n",
      "Epoch 135/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1353 - accuracy: 0.3109\n",
      "Epoch 136/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1706 - accuracy: 0.3491\n",
      "Epoch 137/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1937 - accuracy: 0.3218\n",
      "Epoch 138/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1871 - accuracy: 0.3245\n",
      "Epoch 139/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1768 - accuracy: 0.3218\n",
      "Epoch 140/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0955 - accuracy: 0.3155\n",
      "Epoch 141/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0902 - accuracy: 0.3427\n",
      "Epoch 142/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1633 - accuracy: 0.3209\n",
      "Epoch 143/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1057 - accuracy: 0.3473\n",
      "Epoch 144/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1514 - accuracy: 0.3136\n",
      "Epoch 145/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0882 - accuracy: 0.3127\n",
      "Epoch 146/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2004 - accuracy: 0.3173\n",
      "Epoch 147/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1515 - accuracy: 0.3182\n",
      "Epoch 148/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1684 - accuracy: 0.3182\n",
      "Epoch 149/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1299 - accuracy: 0.3109\n",
      "Epoch 150/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0819 - accuracy: 0.3127\n",
      "Epoch 151/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1473 - accuracy: 0.3182\n",
      "Epoch 152/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0521 - accuracy: 0.3391\n",
      "Epoch 153/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1542 - accuracy: 0.3209\n",
      "Epoch 154/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1035 - accuracy: 0.3155\n",
      "Epoch 155/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0935 - accuracy: 0.3027\n",
      "Epoch 156/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1533 - accuracy: 0.3145\n",
      "Epoch 157/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1412 - accuracy: 0.3245\n",
      "Epoch 158/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1628 - accuracy: 0.3073\n",
      "Epoch 159/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1552 - accuracy: 0.3273\n",
      "Epoch 160/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1560 - accuracy: 0.3182\n",
      "Epoch 161/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1257 - accuracy: 0.3255\n",
      "Epoch 162/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1624 - accuracy: 0.3291\n",
      "Epoch 163/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0659 - accuracy: 0.3536\n",
      "Epoch 164/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1309 - accuracy: 0.3127\n",
      "Epoch 165/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0863 - accuracy: 0.3227\n",
      "Epoch 166/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1188 - accuracy: 0.3245\n",
      "Epoch 167/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1798 - accuracy: 0.3109\n",
      "Epoch 168/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1708 - accuracy: 0.3145\n",
      "Epoch 169/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1531 - accuracy: 0.3109\n",
      "Epoch 170/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0491 - accuracy: 0.3518\n",
      "Epoch 171/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0992 - accuracy: 0.3300\n",
      "Epoch 172/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1531 - accuracy: 0.3109\n",
      "Epoch 173/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1419 - accuracy: 0.3145\n",
      "Epoch 174/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1478 - accuracy: 0.3273\n",
      "Epoch 175/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.2337 - accuracy: 0.2809\n",
      "Epoch 176/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 1.9969 - accuracy: 0.3391\n",
      "Epoch 177/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1777 - accuracy: 0.3100\n",
      "Epoch 178/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0683 - accuracy: 0.3455\n",
      "Epoch 179/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1416 - accuracy: 0.3345\n",
      "Epoch 180/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1204 - accuracy: 0.3282\n",
      "Epoch 181/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0918 - accuracy: 0.3555\n",
      "Epoch 182/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1662 - accuracy: 0.3373\n",
      "Epoch 183/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1262 - accuracy: 0.3336\n",
      "Epoch 184/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0990 - accuracy: 0.3109\n",
      "Epoch 185/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1309 - accuracy: 0.3264\n",
      "Epoch 186/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1082 - accuracy: 0.3518\n",
      "Epoch 187/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1309 - accuracy: 0.3145\n",
      "Epoch 188/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0908 - accuracy: 0.3400\n",
      "Epoch 189/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1145 - accuracy: 0.3418\n",
      "Epoch 190/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1089 - accuracy: 0.3318\n",
      "Epoch 191/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0665 - accuracy: 0.3291\n",
      "Epoch 192/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1033 - accuracy: 0.3318\n",
      "Epoch 193/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0396 - accuracy: 0.3555\n",
      "Epoch 194/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1444 - accuracy: 0.3209\n",
      "Epoch 195/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1128 - accuracy: 0.3473\n",
      "Epoch 196/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0743 - accuracy: 0.3373\n",
      "Epoch 197/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1202 - accuracy: 0.3291\n",
      "Epoch 198/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0929 - accuracy: 0.3345\n",
      "Epoch 199/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.0830 - accuracy: 0.3282\n",
      "Epoch 200/200\n",
      "220/220 [==============================] - 0s 2ms/step - loss: 2.1523 - accuracy: 0.3345\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open('..\\problems_samples.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    new = []\n",
    "    for j in data[i]['keywords']:\n",
    "        new.append(preprocess(j))\n",
    "    data[i]['keywords'] = new\n",
    "\n",
    "train_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 65ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'intent': ' Bomba agitando o fluido e não descarregando a água após um surto inicial e continuando a recircular',\n",
       "  'probability': '0.117946066'}]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from random import choice\n",
    "\n",
    "\n",
    "# retorna 0 ou 1 para cada palavra da bolsa de palavras\n",
    "def bag_of_words(writing, words):\n",
    "    # Pega as sentenças que são limpas e cria um pacote de palavras que são usadas para classes de previsão que são baseadas nos resultados que obtiver treinando o modelo.\n",
    "    sentence_words = writing.split()\n",
    "    # cria uma matriz de N palavras\n",
    "    bag = [0]*len(words)\n",
    "    for setence in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == setence:\n",
    "                # atribui 1 no pacote de palavra se a palavra atual estiver na posição da frase\n",
    "                bag[i] = 1\n",
    "    return(np.array(bag))\n",
    "\n",
    "# Faz a previsao do pacote de palavras, usa como limite de erro 0.25 para evitar overfitting, e classifica esses resultados por força da probabilidade.\n",
    "def class_prediction(input_user, model_path, words_path, classes_path):\n",
    "    model = load_model(model_path)\n",
    "    words = load(open(words_path, 'rb'))\n",
    "    classes = load(open(classes_path, 'rb'))\n",
    "    # filtra as previsões abaixo de um limite 0.25\n",
    "    prevision = bag_of_words(input_user, words)\n",
    "    response_prediction = model.predict(np.array([prevision]))[0]\n",
    "    results = [[index, response] for index, response in enumerate(response_prediction)]\n",
    "    # verifica nas previsões se não há 1 na lista, se não há envia a resposta padrão (anything_else) ou se não corresponde a margem de erro\n",
    "    if \"1\" not in str(prevision) or len(results) == 0 :\n",
    "        results = [[0, response_prediction[0]]]\n",
    "    # classifica por força de probabilidade\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results]\n",
    "\n",
    "\n",
    "text = preprocess(\"Existe alguma forma de ajustar a bomba para reduzir as vibrações que estou sentindo?\")\n",
    "a = class_prediction(text,r'model.h5',r'words.pkl',r'labels.pkl')\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import hashing_trick, text_to_word_sequence\n",
    "\n",
    "# Model Components\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "\n",
    "# Data Splitting\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados de treino\n",
    "train = pd.read_pickle('objects/train.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['samples'], train['Intent'], test_size = 0.3, shuffle = True, stratify = train['Intents'], random_state = 7)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "tokenizer_ = Tokenizer()\n",
    "tokenizer_.fit_on_texts(X_train)\n",
    "print(f\"Train Document Count: \\n{tokenizer_.document_count}\\n\")\n",
    "\n",
    "def convert_to_padded(tokenizer, docs):\n",
    "    embedded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(embedded, maxlen = max_length, padding = 'post')\n",
    "    return padded\n",
    "\n",
    "vocab_size = len(tokenizer_.word_counts) + 1\n",
    "print(f'Vocab size:\\n{vocab_size}')\n",
    "\n",
    "padded_X_train = convert_to_padded(tokenizer = tokenizer_, docs = X_train)\n",
    "padded_X_test = convert_to_padded(tokenizer = tokenizer_, docs = X_test)\n",
    "print(f'padded_X_train\\n{padded_X_train}')\n",
    "print(f'padded_X_val\\n{padded_X_test}')\n",
    "\n",
    "max_length = len(max(padded_X_train, key = len))\n",
    "print(f'Max length:\\n{max_length}')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('models/glove.twitter.27B/glove.twitter.27B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "word_index = tokenizer_.word_index\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_intent_classifier(vocab_size):\n",
    "    # define o modelo\n",
    "    model = Sequential()\n",
    "    labels = []\n",
    "\n",
    "    # Camada de Embedding\n",
    "    model.add(Embedding(vocab_size, embedding_matrix.shape[1], input_length=32, trainable=False, weights=[embedding_matrix]))\n",
    "\n",
    "    # Camada LSTM (camada recorrente)\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "    # Camadas densas\n",
    "    model.add(Dense(224, activation=\"relu\", kernel_regularizer='l2'))\n",
    "    model.add(Dense(224, activation=\"relu\", kernel_regularizer='l2'))\n",
    "\n",
    "    # Camada de dropout para evitar overfitting\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(labels), activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, filename, epoch_num):\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < epoch_num-10:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    lr_sched_checkpoint = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    hist = model.fit(padded_X_train, y_train, epoch_num, batch_size=32, validation_data=(padded_X_test, y_test), callbacks=[checkpoint, lr_sched_checkpoint, early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
