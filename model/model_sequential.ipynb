{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import shuffle\n",
    "from pickle import dump, load\n",
    "from numpy import array\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from re import sub\n",
    "from nltk import download\n",
    "download('punkt')\n",
    "download('stopwords')\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    keywords = ' '.join(keywords)\n",
    "    return keywords\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    stems = []\n",
    "    tokens = word_tokenize(text)\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return str(stems)\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess(text):\n",
    "    text = remove_punct(text)\n",
    "    text = extract_keywords(text)\n",
    "    text = preprocess_stem(text)\n",
    "    text = remove_accent(text)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preparing_documents(list_obj):\n",
    "    words = []\n",
    "    documents = []\n",
    "    for obj in list_obj:\n",
    "        label = obj['label']\n",
    "        samples = obj['keywords']\n",
    "        for sample in samples:\n",
    "            words.extend(sample)\n",
    "            documents.append((sample, label))\n",
    "    return words, documents\n",
    "\n",
    "def train_model1(list_obj):\n",
    "    labels = []\n",
    "    for obj in list_obj:\n",
    "        labels.append(obj['label'])\n",
    "\n",
    "    words, documents = preparing_documents(list_obj)\n",
    "\n",
    "    words = sorted(list(set(words)))\n",
    "    labels = sorted(list(set(labels)))\n",
    "\n",
    "    words_path = \"words.pkl\"\n",
    "    labels_path = \"labels.pkl\"\n",
    "\n",
    "    dump(words, open(words_path, 'wb'))\n",
    "    dump(labels, open(labels_path, 'wb'))\n",
    "\n",
    "    training = []\n",
    "    output_empty = [0] * len(labels)\n",
    "    for document in documents:\n",
    "        bag = []\n",
    "        pattern_words = document[0]\n",
    "        for word in words:\n",
    "            bag.append(1) if word in pattern_words else bag.append(0)\n",
    "        while len(bag) < len(words):\n",
    "            bag.append(0)\n",
    "        output_row = list(output_empty)\n",
    "        output_row[labels.index(document[1])] = 1\n",
    "        training.append([bag, output_row])\n",
    "\n",
    "    shuffle(training)\n",
    "    training = array(training, dtype=object)\n",
    "\n",
    "    x = list(training[:, 0])\n",
    "    y = list(training[:, 1])\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(128, input_shape=(len(x[0]),), activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(labels), activation='softmax'))\n",
    "\n",
    "    sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=sgd, metrics=['accuracy'])\n",
    "\n",
    "    m = model.fit(array(x), array(y), epochs=200, batch_size=5, verbose=1)\n",
    "\n",
    "    model_path = (\"model.h5\")\n",
    "    model.save(model_path, m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open('..\\problems_samples.json','r',encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "for i in range(len(data)):\n",
    "    new = []\n",
    "    for j in data[i]['keywords']:\n",
    "        new.append(preprocess(j))\n",
    "    data[i]['keywords'] = new\n",
    "\n",
    "train_model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from pickle import load\n",
    "from keras.models import load_model\n",
    "from random import choice\n",
    "\n",
    "\n",
    "# retorna 0 ou 1 para cada palavra da bolsa de palavras\n",
    "def bag_of_words(writing, words):\n",
    "    # Pega as sentenças que são limpas e cria um pacote de palavras que são usadas para classes de previsão que são baseadas nos resultados que obtiver treinando o modelo.\n",
    "    sentence_words = writing.split()\n",
    "    # cria uma matriz de N palavras\n",
    "    bag = [0]*len(words)\n",
    "    for setence in sentence_words:\n",
    "        for i, word in enumerate(words):\n",
    "            if word == setence:\n",
    "                # atribui 1 no pacote de palavra se a palavra atual estiver na posição da frase\n",
    "                bag[i] = 1\n",
    "    return(np.array(bag))\n",
    "\n",
    "# Faz a previsao do pacote de palavras, usa como limite de erro 0.25 para evitar overfitting, e classifica esses resultados por força da probabilidade.\n",
    "def class_prediction(input_user, model_path, words_path, classes_path):\n",
    "    model = load_model(model_path)\n",
    "    words = load(open(words_path, 'rb'))\n",
    "    classes = load(open(classes_path, 'rb'))\n",
    "    # filtra as previsões abaixo de um limite 0.25\n",
    "    prevision = bag_of_words(input_user, words)\n",
    "    response_prediction = model.predict(np.array([prevision]))[0]\n",
    "    results = [[index, response] for index, response in enumerate(response_prediction)]\n",
    "    # verifica nas previsões se não há 1 na lista, se não há envia a resposta padrão (anything_else) ou se não corresponde a margem de erro\n",
    "    if \"1\" not in str(prevision) or len(results) == 0 :\n",
    "        results = [[0, response_prediction[0]]]\n",
    "    # classifica por força de probabilidade\n",
    "    results.sort(key=lambda x: x[1], reverse=True)\n",
    "    return [{\"intent\": classes[r[0]], \"probability\": str(r[1])} for r in results]\n",
    "\n",
    "\n",
    "text = preprocess(\"Existe alguma forma de ajustar a bomba para reduzir as vibrações que estou sentindo?\")\n",
    "a = class_prediction(text,r'model.h5',r'words.pkl',r'labels.pkl')\n",
    "a"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Novo modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Science\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "\n",
    "# Deep Learning\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Preprocessing\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "from keras.preprocessing.text import Tokenizer, one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.text import hashing_trick, text_to_word_sequence\n",
    "\n",
    "# Model Components\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, LSTM, Bidirectional, Embedding, Dropout\n",
    "\n",
    "# Data Splitting\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lendo os dados de treino\n",
    "train = pd.read_pickle('objects/train.pkl')\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(train['samples'], train['Intent'], test_size = 0.3, shuffle = True, stratify = train['Intents'], random_state = 7)\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "y_test = le.transform(y_test)\n",
    "\n",
    "tokenizer_ = Tokenizer()\n",
    "tokenizer_.fit_on_texts(X_train)\n",
    "print(f\"Train Document Count: \\n{tokenizer_.document_count}\\n\")\n",
    "\n",
    "def convert_to_padded(tokenizer, docs):\n",
    "    embedded = tokenizer.texts_to_sequences(docs)\n",
    "    padded = pad_sequences(embedded, maxlen = max_length, padding = 'post')\n",
    "    return padded\n",
    "\n",
    "vocab_size = len(tokenizer_.word_counts) + 1\n",
    "print(f'Vocab size:\\n{vocab_size}')\n",
    "\n",
    "padded_X_train = convert_to_padded(tokenizer = tokenizer_, docs = X_train)\n",
    "padded_X_test = convert_to_padded(tokenizer = tokenizer_, docs = X_test)\n",
    "print(f'padded_X_train\\n{padded_X_train}')\n",
    "print(f'padded_X_val\\n{padded_X_test}')\n",
    "\n",
    "max_length = len(max(padded_X_train, key = len))\n",
    "print(f'Max length:\\n{max_length}')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('models/glove.twitter.27B/glove.twitter.27B.50d.txt')\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "word_index = tokenizer_.word_index\n",
    "EMBEDDING_DIM = 50\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_intent_classifier(vocab_size):\n",
    "    # define o modelo\n",
    "    model = Sequential()\n",
    "    labels = []\n",
    "\n",
    "    # Camada de Embedding\n",
    "    model.add(Embedding(vocab_size, embedding_matrix.shape[1], input_length=32, trainable=False, weights=[embedding_matrix]))\n",
    "\n",
    "    # Camada LSTM (camada recorrente)\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "\n",
    "    # Camadas densas\n",
    "    model.add(Dense(224, activation=\"relu\", kernel_regularizer='l2'))\n",
    "    model.add(Dense(224, activation=\"relu\", kernel_regularizer='l2'))\n",
    "\n",
    "    # Camada de dropout para evitar overfitting\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(len(labels), activation=\"softmax\"))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, filename, epoch_num):\n",
    "    model.compile(loss=\"sparse_categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "    def scheduler(epoch, lr):\n",
    "        if epoch < epoch_num-10:\n",
    "            return lr\n",
    "        else:\n",
    "            return lr * tf.math.exp(-0.1)\n",
    "\n",
    "    lr_sched_checkpoint = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "    early_stopping = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', min_delta=0, patience=3, verbose=0, mode='auto',\n",
    "    baseline=None, restore_best_weights=True\n",
    "    )\n",
    "\n",
    "    checkpoint = ModelCheckpoint(filename, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    hist = model.fit(padded_X_train, y_train, epoch_num, batch_size=32, validation_data=(padded_X_test, y_test), callbacks=[checkpoint, lr_sched_checkpoint, early_stopping])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
