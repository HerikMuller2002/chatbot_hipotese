{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import pandas as pd\n",
    "\n",
    "from re import sub\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from spacy.lang.pt.stop_words import STOP_WORDS\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_num(text):\n",
    "    text = sub(r'\\d+', '', text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def remove_punct(text):\n",
    "    text = sub(r\"[!#$%&'()*+,-./:;<=>?@[^_`{|}~]+\", ' ',text)\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def extract_keywords(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    keywords = []\n",
    "    for word in tokens:\n",
    "        word = word.lower()\n",
    "        if word not in stopwords.words('portuguese') or word.lower() not in STOP_WORDS:\n",
    "            keywords.append(word)\n",
    "    return keywords\n",
    "\n",
    "def get_synonyms(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    synonyms = []\n",
    "    for word in tokens:\n",
    "        for syn in wordnet.synsets(word, lang=\"por\"):\n",
    "            for lemma in syn.lemmas(lang=\"por\"):\n",
    "                synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def preprocess_lemma(text):\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmas = []\n",
    "    for token in tokens:\n",
    "        lemmas.append(lemmatizer.lemmatize(token))\n",
    "    lemmas = ' '.join(lemmas)\n",
    "    return lemmas\n",
    "\n",
    "def remove_accent(text):\n",
    "    text = sub('[áàãâä]', 'a', sub('[éèêë]', 'e', sub('[íìîï]', 'i', sub('[óòõôö]', 'o', sub('[úùûü]', 'u', text)))))\n",
    "    text = sub(r'\\s+', ' ',text)\n",
    "    return text\n",
    "\n",
    "def preprocess_stem(text):\n",
    "    stemmer = SnowballStemmer(\"portuguese\")\n",
    "    tokens = word_tokenize(text)\n",
    "    stems = []\n",
    "    for token in tokens:\n",
    "        stems.append(stemmer.stem(token))\n",
    "    stems = ' '.join(stems)\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_excel(r'..\\portugues\\pt_troubleshooting.xlsx')v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_rows = []\n",
    "for row in df['problem'].unique():\n",
    "    list_words = []\n",
    "    row = remove_num(row)\n",
    "    row = remove_punct(row)\n",
    "    list_keywords = extract_keywords(row)\n",
    "    for keyword in list_keywords:                \n",
    "        x = remove_accent(keyword)\n",
    "        if x not in list_words:\n",
    "            list_words.append(x)\n",
    "\n",
    "        x = preprocess_lemma(x)\n",
    "        if x not in list_words:\n",
    "            list_words.append(x)\n",
    "            \n",
    "        x = preprocess_stem(x)\n",
    "        if x not in list_words:\n",
    "            list_words.append(x)\n",
    "        \n",
    "        z = get_synonyms(keyword)\n",
    "        for synonym in z:\n",
    "            synonym = remove_punct(synonym)\n",
    "            x = remove_accent(synonym)\n",
    "            if x not in list_words:\n",
    "                list_words.append(x)\n",
    "\n",
    "            x = preprocess_lemma(x)\n",
    "            if x not in list_words:\n",
    "                list_words.append(x)\n",
    "                \n",
    "            x = preprocess_stem(x)\n",
    "            if x not in list_words:\n",
    "                list_words.append(x)\n",
    "    list_rows.append(list_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(list_rows)):\n",
    "    list_rows[i] = [', '.join(list_rows[i])]\n",
    "new_df = pd.DataFrame(list_rows, columns=['patterns'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df.to_excel(r'..\\portugues\\pt_patterns.xlsx')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
